{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc640239-306f-4d7d-bd13-d7beb290f752",
   "metadata": {},
   "source": [
    "<h3>Sammlung von nützlichen Funktionen und Allgemeines zu Sklearn.</h3>\n",
    "- Als Erstes als Notebook, wird evtl. als PDF oder Bild erstellt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3144050-f84d-441d-843e-5b84fa8ad949",
   "metadata": {},
   "source": [
    "Diese Sammlung ermöglicht es ein schnelles nachschlagen von Methoden und erklärt ganz kurz den Zweck. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6876800-d826-4412-b566-f2dc6b590640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "27dc7394-866e-489a-a54c-edab6b2536f3",
   "metadata": {},
   "source": [
    "## Modelle ##\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.tree     import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.naive_bayes  import GaussianNB\n",
    "from sklearn.naive_bayes  import BernoulliNB\n",
    "\n",
    "from sklearn.neighbors    import  KNeighborsClassifier / KNeighborsRegressor\n",
    "from sklearn.svm          import SVC / SVR\n",
    "\n",
    "from sklearn.cluster      import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9f9b8-0632-47dd-86c9-a63d08b25705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c4ff9840-9da6-4107-a7db-c3dc71a26c79",
   "metadata": {},
   "source": [
    "## Plot Decision Boundary ## \n",
    "# - 2D\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Model.\n",
    "model = KNeighborsClassifier(n_neighbors=3).fit(X, y)\n",
    "# Skalar für Plot. \n",
    "skalar = [min(X['sepal length (cm)']), max(X['sepal length (cm)']),\\\n",
    "          min(X['sepal width (cm)']), max(X['sepal width (cm)'])]\n",
    "\n",
    "_, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Plot für Felder. \n",
    "common_params = {\"estimator\": model, \"X\": X, \"ax\": ax}\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "        **common_params,\n",
    "        response_method=\"auto\",    # predict bei 2 Klassen\n",
    "        plot_method=\"pcolormesh\",  # {'contourf', 'contour', 'pcolormesh'}\n",
    "        alpha=0.3,\n",
    "    )\n",
    "# Plot Boundary.\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "            **common_params,\n",
    "            response_method=\"auto\",  # decision_function bei 2 Klassen\n",
    "            plot_method=\"contour\",\n",
    "            levels=[-1, 0, 1],\n",
    "            colors=[\"k\", \"k\", \"k\"],\n",
    "            linestyles=[\"--\", \"-\", \"--\",],\n",
    "        )\n",
    "# Plot der Datenpunkte.\n",
    "ax.scatter(X['sepal length (cm)'], X['sepal width (cm)'], c=y, s=30, edgecolors=\"k\")\n",
    "ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Classes\")\n",
    "\n",
    "# Minimal: #\n",
    "xx, yy = np.meshgrid(np.arange(minx, maxx, 0.02), np.arange(miny, maxy, 0.02))\n",
    "# Model.\n",
    "model = <...>\n",
    "\n",
    "# Predictions.\n",
    "pred = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "pred = pred.reshape(xx.shape)\n",
    "\n",
    "# Plot.\n",
    "plt.contourf(xx, yy, pred, alpha=0.3, cmap=plt.cm.coolwarm)\n",
    "plt.scatter(X, y, c=y, edgecolors='k', cmap=plt.cm.coolwarm)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77323e35-e6d6-4809-8706-ece2feae0e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "1d50c354-d82a-410f-a427-cce8209eae11",
   "metadata": {},
   "source": [
    "## Cross-Validation ##\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Teile Dataset in n-teile auf und nutze subsets für den Score. \n",
    "cross_validate(model, X_train, y_train, cv=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff35d0c9-b622-4381-bc3e-1de54e35c884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "1d31c850-7ef0-4a33-9eda-b6456ed8d939",
   "metadata": {},
   "source": [
    "## Grid-Search ## linear, random.\n",
    "# - Finde optimale Parameter für das Model - Hyperparameter Search. \n",
    "\n",
    "# Linear # \n",
    "gsc = GridSearchCV( model , { \n",
    "    'param_1': [1, 10, 10],\n",
    "    'param_2': ['rbf','linear'],\n",
    "}, cv=5, return_train_score=False)  # Mit Cross-Validation 5\n",
    "gsc.fit(X_train, y_train)\n",
    "gsc.best_score_\n",
    "gsc.best_params_\n",
    "\n",
    "# Random # \n",
    "rdm_gridCV = RandomizedSearchCV( model , {\n",
    "    'param_1': [1, 10, 10],\n",
    "    'param_2': ['rbf','linear'],\n",
    "}, cv=5, return_train_score=False, n_iter=20)  # Mit Cross-Validation 5 und 20 Iterationen.\n",
    "rdm_gridCV.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b557f3d5-472c-46c4-88be-2d6c076335e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "1a879b10-4d95-4a42-8372-62f5621c289f",
   "metadata": {},
   "source": [
    "## Model-Selection ## mit Grid-Search\n",
    "\n",
    "# Erstelle Dict.: \n",
    "data_dict = {\n",
    "    'svc':{                # Um welches Model es sich handelt. \n",
    "        'model': SVC(),    # Model, das verwendet werden soll ggf. mit Startparameter.\n",
    "        'model_param':{    # Verschiedene Paramter als Dict, damit diese gemeinsam plaziert werden. \n",
    "            'C': [1,10,20],             \n",
    "            'kernel': ['rbf','linear', 'poly', 'sigmoid'],\n",
    "            'degree': [2, 3, 4, 5]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'gaussianNB':{\n",
    "        'model': GaussianNB(),  \n",
    "        'model_param':{   \n",
    "            'var_smoothing':[1e-9, 1e-8, 1e-7]             \n",
    "        }\n",
    "    },\n",
    "   '....':\n",
    "\n",
    "# Durchlauf mit allen Modellen. \n",
    "result = []\n",
    "for model, param in data_dict.items():\n",
    "\n",
    "    gsc = GridSearchCV( param['model'], param['model_param'], cv=5, return_train_score=False)\n",
    "    gsc.fit(X_train, y_train)\n",
    "\n",
    "    # Das kann dann später als Dataframe dargestellt werden. \n",
    "    result.append({         \n",
    "        'model_name':       model,\n",
    "        'best_model_score': gsc.best_score_,\n",
    "        'best_params':      gsc.best_params_\n",
    "    })\n",
    "best_modelparameters_df = pd.DataFrame(result, columns=list(result[0].keys()) )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ede82-aa73-48ca-9641-ecebca2a8ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "af55a621-ce18-4bd8-98f2-3e81a36cf4a3",
   "metadata": {},
   "source": [
    "## PCA ## \n",
    "# - Reduziere Dimension der Features.\n",
    "\n",
    "from sklearn.decomposition   import PCA     \n",
    "pca   = PCA(n_components=n)  # n Features sollen übrigbleiben. \n",
    "pca   = PCA(.90)             # 90% der Features die den meisten Einfluss haben. \n",
    "X_PCA = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bcbf9d-6ef0-4339-9e3d-d7975e6f2747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b36318b7-8437-4fd1-8bd3-59cfe0f737d3",
   "metadata": {},
   "source": [
    "## Ensemble - Bagging ## \n",
    "# - Mehrere Algorithmen für eine Antwort verwenden -> wie Random-Forest\n",
    "\n",
    "# Bagging - Resampling with Replacement.\n",
    "# - Modelle sind Weak-Learners \n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "model = BaggingClassifier( \n",
    "    estimator= model,      # Welches Model. \n",
    "    n_estimators  = 10,    # Wie viele Modelle => n-Subsets -> n-Modelle\n",
    "    max_samples   = 0.85,  # Pool für Subsets => 100 Samples und max_samples=0.85 -> Subset hat 85 Samples. \n",
    "    oob_score     = True   # Out of Bag. Wenn Datenpunk nicht gezogen wird => Nutze es als Testset. Nur bei Bootstrap. \n",
    "    # verbose       = 2    # Zusätzliche Informationen => kann viel Text generieren\n",
    ")\n",
    "model.fit(X_data, heart_data['HeartDisease'])\n",
    "model.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850fd33d-5dd6-408e-a8af-8eea856c5755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "43bb1c50-daaa-4764-bbec-566e63d7b5d5",
   "metadata": {},
   "source": [
    "## Label-Encoder und On-Hot Encode ## \n",
    "# - Encode kategorische Daten. \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder  -> [0, 1, 2, 3, ...]\n",
    "from sklearn.preprocessing import OneHotEncoder -> [0, 1]\n",
    "# Label-Encoder\n",
    "label_enc     = LabelEncoder()\n",
    "encoded_data  = label_enc.fit_transform(<Column>)\n",
    "# On-Hot Encode\n",
    "sklearn_ohe     = OneHotEncoder(sparse_output=False)   # sparse_output: Wenn True, ist die Ausgabe von Typ:  scipy.sparse.csr_matrix\n",
    "encoded         = sklearn_ohe.fit_transform(<column>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98bb86a-ff20-4873-9e62-2fd449a8b9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "af06d3df-5153-47bd-aa3e-12bd31f37b7f",
   "metadata": {},
   "source": [
    "## Train test split ## \n",
    "# - Teilt Daten auf in Train- und Testset.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Parameter:\n",
    "# - test_size=0.2: Datasets 20% Testdaten.\n",
    "# - stratify=y   : Gleichmäßige Aufteilung der Klassen. \n",
    "# - shuffle=True : Mische Dataset vor der Aufteilung.\n",
    "# - random_state : Steuert Shuffe, wenn None -> Shuffle immer anders.\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa7698-5f6c-4626-a447-05785e65a2ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "9c944b77-d4a7-4a13-99b9-335a8db6c909",
   "metadata": {},
   "source": [
    "## Scaler ## \n",
    "# - Skaliere Daten vor dem Training.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Daten mit Normalverteilung. \n",
    "# - Formel: z = (x - u) / s,  u: std, s: mean. \n",
    "std_scaler  = StandardScaler()\n",
    "scaled_data = std_scaler.fit_transform(df[[columns]])\n",
    "\n",
    "# Daten die nicht normalverteilt sind. \n",
    "# - Formel:    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "# -            X_scaled = X_std * (max - min) + min\n",
    "minmax_scaler = MinMaxScaler( feature_range=(0, 1) )  #  feature_range=(a, b)\n",
    "scaled_data  = minmax_scaler.fit_transform(df[[columns]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d3c7f-217e-4719-b36f-bb40ed0d48ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe9954-bf67-4871-9cb9-900eee2676d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c174e19-3d5c-4583-8a57-9a7a2dd81e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba56f5-e597-4764-a6ca-c827e04e6a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dde508-220c-48f9-9f01-1537b6fd290c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e1f5e-e79f-4e53-a965-6d9449207595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e8382-77a5-48e0-8d3a-8466d4ce3646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
