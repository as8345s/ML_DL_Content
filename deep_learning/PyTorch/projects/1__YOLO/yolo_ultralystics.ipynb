{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b38b4024-8398-4518-bd1c-3a41365f8b64",
   "metadata": {},
   "source": [
    "<h1>YOLO</h1>\n",
    "[Details zur Funktionsweise von YOLO im anderen Notebook]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19920145-2279-4de0-923f-6095f1f0cd0e",
   "metadata": {},
   "source": [
    "<h2>1. Install</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca91060-a4a1-4408-8e9d-62be9bf53b1e",
   "metadata": {},
   "source": [
    "Um nicht alles per Hand zu programmieren, gibt es Libraries, die eine bequeme Nutzung von YOLO erlauben.\n",
    "\n",
    "Als Erstes muss PyTorch installiert werden. <br>\n",
    "> https://pytorch.org/get-started/locally/ [Letzter Zugriff: 08.07.2024]\n",
    "\n",
    "Folge diesen Anweisungen, um Ultralytics zu installieren: <br>\n",
    "> https://github.com/ultralytics/ultralytics [Letzter Zugriff: 08.07.2024] <br>\n",
    "> https://docs.ultralytics.com/quickstart/   [Letzter Zugriff: 08.07.2024]\n",
    "\n",
    "\n",
    "Danach kann dieser Befehl in die Konsole eingegeben werden: <br>\n",
    "`yolo predict model=yolov8s.pt source='https://ultralytics.com/images/bus.jpg'` <br>\n",
    "\n",
    "Damit wird ein Bild heruntergeladen und direkt für die Predicion genutzt. Dabei wird ein Pre-Trained Model verwendet.\n",
    "\n",
    "<u>Hinweis</u>:<br>\n",
    "Um diese Prediction auszuführen, müssen alle Libraries verfügbar sein. <br>\n",
    "Nicht vorhandene Libraries können z. B. mit `pip install <package>==<version>` installiert werden. <br>\n",
    "- Bei Numpy kann der folgende Fehler auftreten: `ImportError: numpy.core.multiarray failed to import`. Um das zu beheben muss eine spezifische Numpy Version installiert werden (hier: 1.26.0 statt 2.0.0).\n",
    "- Dasselbe gilt auf für andere Packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd87fb80-6281-4e97-aa23-d6c5fa886f1b",
   "metadata": {},
   "source": [
    "<h2>Predict</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8d17ac0-a013-43a3-ad93-76a3d23abd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle imports.\n",
    "from ultralytics import YOLO\n",
    "import cv2 as cv \n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a718025a-df98-4f74-a4f2-6de2e3fd5f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.51 ðŸš€ Python-3.12.4 torch-2.3.1+cpu CPU (Intel Core(TM) i9-9900K 3.60GHz)\n",
      "YOLOv8s summary (fused): 168 layers, 11156544 parameters, 0 gradients, 28.6 GFLOPs\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\bus.jpg: 640x480 4 persons, 1 bus, 345.8ms\n",
      "Speed: 6.0ms preprocess, 345.8ms inference, 11.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mruns\\detect\\predict3\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "# Befehl kann auch direkt im Notebook ausgeführt werden.\n",
    "!yolo predict model=yolov8s.pt source='https://ultralytics.com/images/bus.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd34f1-d68a-420d-9f70-cd18facf286f",
   "metadata": {},
   "source": [
    "Das Ergebnis: <br>\n",
    "\n",
    "Linkes Bild: Iput <br>\n",
    "Rechtes Bild: Output<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc528ef-deb1-4cc8-9a54-7e0319efbcb8",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "      <td>\n",
    "      <img src=\"bus.jpg\" hight=300 width=300>\n",
    "      </td>\n",
    "      <td>\n",
    "        <img src=\"./runs/detect/predict/bus.jpg\"  hight=300 width=300>\n",
    "      </td>\n",
    "     </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7f4f5-aeb5-4144-b187-ae2eb4ed344e",
   "metadata": {},
   "source": [
    "Wie geht das?: <br>\n",
    "Das Netzwerk gibt ein Vektor aus, wo unter anderem die Daten für die Boundaries stehen. Diese Müssen nach Klassen in das Bild eingezeichnet werden. <br>\n",
    "Vor dem Einzeichnen wird ein Algorithmus names <u>IOU</u> verwendet, um überlappenden Boxen zu einer zu verschmelzen.\n",
    "- Zudem werden noch andere Daten ausgegeben, wie: 4 persons, 1 bus, ..., siehe unten.\n",
    "\n",
    "Das alles passiert intern, und als Ausgabe bekommen wir das Bild. Die eigentliche Ausgabe des Netzes ist ein Vektor, bestehend aus mehreren sogenannten <u>Anchor Boxen<u>. <br>\n",
    "\n",
    "Siehe auch: <br>\n",
    "> YOLOv3 From Scratch Using PyTorch: <br>\n",
    "> https://www.geeksforgeeks.org/yolov3-from-scratch-using-pytorch/ [Letzter Zugriff: 08.07.2024]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed1d182-339d-4cc0-8893-e0d2ce8b6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Als Code: ## \n",
    "\n",
    "# Lade Model Typ Nano. \n",
    "# - 8n: YOLO Nano, es gibt noch s: small, L: Large und x: extra large.\n",
    "# - Pre-Trained mit COCO Dataset.\n",
    "model = YOLO(\"yolov8n.pt\")  \n",
    "\n",
    "# Führe Prediction aus. \n",
    "results = model(\"bus.jpg\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4421d3b-bfb4-4737-9540-dc7851d272d6",
   "metadata": {},
   "source": [
    "<h2>OpenCV Kamera</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beecbacc-e046-46dd-88d9-476d49d624e0",
   "metadata": {},
   "source": [
    "Wir wollen ja nicht nur einzelne Bilder für die Predictions nutzen, sonders auch Videos und Livestreams.\n",
    "\n",
    "Für diesen Aufbau nutzen wir eine einfache USB Kamera, die über den PC angeschlossen wird. <br>\n",
    "Dank OpenCV können wir diese ganz einfach nutzen.\n",
    "\n",
    "Siehe auch: <br>\n",
    "https://docs.opencv.org/3.4/dd/d43/tutorial_py_video_display.html [Letzter Zugriff: 08.07.2024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1250e235-528c-4b2e-a621-91db63917f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Keine Kamera bei der Quelle: 1\n"
     ]
    }
   ],
   "source": [
    "# #  Kamera da? # # \n",
    "\n",
    "# Teste Quelle.\n",
    "import cv2 as cv \n",
    "\n",
    "def test_device(source):\n",
    "   cap = cv.VideoCapture(source)  # Source: Standardmäßig ist 0 die Kamera. \n",
    "   if cap is None or not cap.isOpened():\n",
    "       print(f'Warning: Keine Kamera bei der Quelle: {source}')\n",
    "\n",
    "test_device(0)\n",
    "test_device(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "920734d4-0943-452e-8d11-a12e1affcafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Öffne Videokanal und Filme # # \n",
    "\n",
    "# Öffne Videokanal. \n",
    "vid = cv.VideoCapture(0) \n",
    "\n",
    "if not vid.isOpened():\n",
    " print(\"Kamera nicht verfügbar\")\n",
    " exit()\n",
    "  \n",
    "while(True): \n",
    "      \n",
    "    # Erfasse Video:\n",
    "    ret, frame = vid.read() \n",
    "  \n",
    "    # Öffne Fenster und zeige Frame. \n",
    "    cv.imshow('frame', frame) \n",
    "\n",
    "    # Durch Taste 'q' soll der Stream beendet werden. \n",
    "    if cv.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "  \n",
    "# Freigeben der Kamera. \n",
    "vid.release() \n",
    "# Schließe alle Fenster. \n",
    "cv.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad306579-f46e-4ec7-aa30-d68fac118c41",
   "metadata": {},
   "source": [
    "Jetzt wollen wir ein Video mit der Kamera aufnehmen und speichern. Danach soll das gespeicherte Video geladen und für die Prediction genutzt werden.\n",
    "\n",
    "Dabei stellen wir ein, dass die Aufnahme n-Sekunden gehen soll.\n",
    "\n",
    "<u>Hinweis</u>:<br>\n",
    "Der Codec 'H264' ermöglicht es das Video später in dem Notebook abzuspielen.<br>\n",
    "Es gibt verschiedene Codecs die verschiedene Eigenschaften haben. Nicht alle können dann als Video im Notebook abgespielt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7abf8779-af4b-417b-a3c1-173e717def58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Nehme Video auf und speichere # # \n",
    "\n",
    "time_limit  = 15\n",
    "fps         = 27.0\n",
    "resolution  = (640, 480)\n",
    "video_path  = './data/output.mp4'\n",
    "\n",
    "vid = cv.VideoCapture(0)\n",
    "if not vid.isOpened():\n",
    "   print(\"Kamera nicht verfügbar\")\n",
    "   exit()\n",
    "    \n",
    "# Erstelle VideoWriter Objekt. \n",
    "fourcc = cv.VideoWriter_fourcc(*'H264')  #  mp4v(.mp4, )XVID(.avi), MJPG(.mp4) Weitere Formate auf der OpenCV Webseite. \n",
    "out = cv.VideoWriter(video_path, fourcc, fps, resolution)   # VideoWriter(Datei, fourcc, FPS, res( , ) )\n",
    "\n",
    "# Starte Timer.\n",
    "start_time = time.time()\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "\n",
    "    if ret:\n",
    "        \n",
    "        #frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        \n",
    "        out.write(frame)\n",
    "        # Öffne Fenster und zeige Frame.\n",
    "        cv.imshow('frame', frame)\n",
    "        # Limitieren Aufnahmezeit. \n",
    "        if time.time() - start_time > time_limit:\n",
    "            break\n",
    "        # Drücke Taste zum aufhören. \n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "vid.release()\n",
    "out.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ffcee-ff8a-44e1-aad9-cce1cf8dffa3",
   "metadata": {},
   "source": [
    "Wir können testet, ob diese Datei geschrieben wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ed4885b-0567-4d13-b951-da79883c2b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bbb602f-d6d3-4ca3-9cb3-bcdc5ae25bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Größe:    15704499 bytes\n",
      "Erstellt: Fri Jul 12 11:43:30 2024\n"
     ]
    }
   ],
   "source": [
    "print(f\"Größe:    {os.stat(video_path).st_size} bytes\")\n",
    "print(f\"Erstellt: {time.ctime(os.stat(video_path).st_ctime)}\")\n",
    "# ggf. weitere Informationen ausgeben... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8e06c-7c3e-48dd-84b4-a776757a7c09",
   "metadata": {},
   "source": [
    "Jetzt kann das Video geladen und abgespielt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24384181-1113-4f74-ade5-f42fcf137b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keine Frames mehr erhalten\n"
     ]
    }
   ],
   "source": [
    "# # Playback des aufgenommenen Videos # # \n",
    "\n",
    "cap = cv.VideoCapture(video_path)\n",
    "\n",
    "while cap.isOpened():\n",
    "     ret, frame = cap.read()\n",
    "    \n",
    "     if not ret:\n",
    "         print(\"Keine Frames mehr erhalten\")\n",
    "         break\n",
    "     # Zeige in einem Fenster das aufgenommene Video.\n",
    "     cv.imshow('frame', frame)\n",
    "     if cv.waitKey(1) == ord('q'):\n",
    "         break\n",
    "         \n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c6df76-5856-4ef6-97d8-583e0c1f5b2d",
   "metadata": {},
   "source": [
    "Alternativ können wir auch Videos hier in dem Notebook einbinden, was ganz nett sein kann. Mit weiteren CSS und HTML Elementen könnte man das weiter ausbauen...\n",
    "- In Jupyter Notebooks können CSS und HTML Elemente genutzt werden, um Inhalte benutzerdefiniert darzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2eed9e-c9c5-4688-b78f-97744df390eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"data/output.mp4\", embed=True, width=300, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d6cdc-744f-44de-9690-05b1f6bf93d7",
   "metadata": {},
   "source": [
    "<h2>Object Detection mit gespeicherten Videos</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6da20e-3fda-4353-b2df-5d8301c825ae",
   "metadata": {},
   "source": [
    "Mit dem angegebenen Model kann das Video, was wir erstellt haben, jetzt die Inference verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bc920dc-9df9-42b2-9655-344f61b1f0a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "WARNING  inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "video 1/1 (frame 1/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 155.1ms\n",
      "video 1/1 (frame 2/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 140.5ms\n",
      "video 1/1 (frame 3/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 140.0ms\n",
      "video 1/1 (frame 4/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 5/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 6/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 7/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 8/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 140.9ms\n",
      "video 1/1 (frame 9/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 10/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 11/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.6ms\n",
      "video 1/1 (frame 12/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 13/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 14/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.1ms\n",
      "video 1/1 (frame 15/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 16/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.1ms\n",
      "video 1/1 (frame 17/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 18/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 133.0ms\n",
      "video 1/1 (frame 19/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 20/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 21/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 22/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.2ms\n",
      "video 1/1 (frame 23/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 24/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 25/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 26/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 27/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 28/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 29/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 142.0ms\n",
      "video 1/1 (frame 30/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 133.0ms\n",
      "video 1/1 (frame 31/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 32/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 33/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 34/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 35/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 134.0ms\n",
      "video 1/1 (frame 36/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 133.0ms\n",
      "video 1/1 (frame 37/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 38/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 39/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 137.0ms\n",
      "video 1/1 (frame 40/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 41/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 154.0ms\n",
      "video 1/1 (frame 42/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 43/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 44/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 45/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 134.0ms\n",
      "video 1/1 (frame 46/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 134.0ms\n",
      "video 1/1 (frame 47/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 133.0ms\n",
      "video 1/1 (frame 48/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 134.0ms\n",
      "video 1/1 (frame 49/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 50/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 51/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 52/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 132.0ms\n",
      "video 1/1 (frame 53/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 54/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 132.0ms\n",
      "video 1/1 (frame 55/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 56/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.9ms\n",
      "video 1/1 (frame 57/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 134.0ms\n",
      "video 1/1 (frame 58/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 133.0ms\n",
      "video 1/1 (frame 59/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 60/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 61/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 62/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 63/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 64/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 65/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 66/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 67/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 68/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 69/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 70/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 71/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 72/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 73/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 133.0ms\n",
      "video 1/1 (frame 74/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 75/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 76/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 77/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 133.0ms\n",
      "video 1/1 (frame 78/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 133.0ms\n",
      "video 1/1 (frame 79/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.1ms\n",
      "video 1/1 (frame 80/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 81/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 82/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 83/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 84/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 85/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 86/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 133.0ms\n",
      "video 1/1 (frame 87/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 88/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 89/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 90/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 143.0ms\n",
      "video 1/1 (frame 91/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 132.0ms\n",
      "video 1/1 (frame 92/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 133.0ms\n",
      "video 1/1 (frame 93/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 94/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 95/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 96/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 97/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 98/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 99/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 100/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 101/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 102/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 103/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 104/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 105/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 106/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 133.0ms\n",
      "video 1/1 (frame 107/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 108/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 109/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 110/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 111/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 112/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 147.0ms\n",
      "video 1/1 (frame 113/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 114/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 115/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 116/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 117/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 118/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 119/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 120/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 135.0ms\n",
      "video 1/1 (frame 121/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 122/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 123/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 124/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 125/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 126/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 127/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 128/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 129/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 130/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 131/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 132/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 133/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 134/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 135/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.2ms\n",
      "video 1/1 (frame 136/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 138.3ms\n",
      "video 1/1 (frame 137/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.1ms\n",
      "video 1/1 (frame 138/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.1ms\n",
      "video 1/1 (frame 139/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 142.2ms\n",
      "video 1/1 (frame 140/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 133.7ms\n",
      "video 1/1 (frame 141/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 142/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 143/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 144/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 145/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 146/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 147/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 148/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 149/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 150/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 151/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 152/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 153/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 154/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 155/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 156/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 157/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 158/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 159/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 160/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 161/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 162/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 163/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 164/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 165/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 166/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 167/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 168/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 169/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 170/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 3 persons, 138.0ms\n",
      "video 1/1 (frame 171/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 138.0ms\n",
      "video 1/1 (frame 172/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 136.0ms\n",
      "video 1/1 (frame 173/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 137.0ms\n",
      "video 1/1 (frame 174/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 137.0ms\n",
      "video 1/1 (frame 175/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 176/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 177/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 178/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 142.0ms\n",
      "video 1/1 (frame 179/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 180/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 181/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 182/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 133.0ms\n",
      "video 1/1 (frame 183/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 184/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 185/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 186/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 187/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 188/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 189/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 190/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 191/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 192/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 140.0ms\n",
      "video 1/1 (frame 193/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 194/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 195/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 196/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 197/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 198/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 199/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 200/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 201/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 202/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 203/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 204/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 205/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 206/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 207/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 208/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 143.0ms\n",
      "video 1/1 (frame 209/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 210/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 211/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 138.0ms\n",
      "video 1/1 (frame 212/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 213/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 214/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 215/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 216/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 217/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 135.0ms\n",
      "video 1/1 (frame 218/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 135.0ms\n",
      "video 1/1 (frame 219/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 3 persons, 135.1ms\n",
      "video 1/1 (frame 220/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.6ms\n",
      "video 1/1 (frame 221/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 135.1ms\n",
      "video 1/1 (frame 222/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 223/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 134.8ms\n",
      "video 1/1 (frame 224/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 137.0ms\n",
      "video 1/1 (frame 225/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 135.0ms\n",
      "video 1/1 (frame 226/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 134.3ms\n",
      "video 1/1 (frame 227/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 1 cell phone, 132.0ms\n",
      "video 1/1 (frame 228/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 134.0ms\n",
      "video 1/1 (frame 229/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 230/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 231/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 1 kite, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 232/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 233/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 3 persons, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 234/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 235/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 236/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 237/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 238/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 141.0ms\n",
      "video 1/1 (frame 239/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 240/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 241/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 242/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 243/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 244/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 245/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 246/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 247/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 138.0ms\n",
      "video 1/1 (frame 248/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 249/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 250/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 251/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.2ms\n",
      "video 1/1 (frame 252/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.3ms\n",
      "video 1/1 (frame 253/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 254/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 138.0ms\n",
      "video 1/1 (frame 255/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 256/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 257/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 258/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 259/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 260/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 138.0ms\n",
      "video 1/1 (frame 261/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 262/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 263/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 264/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 265/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 266/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 267/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 268/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 269/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 142.0ms\n",
      "video 1/1 (frame 270/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 271/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 272/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 273/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 274/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 133.0ms\n",
      "video 1/1 (frame 275/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 276/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 277/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 278/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 279/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 138.0ms\n",
      "video 1/1 (frame 280/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 281/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 139.0ms\n",
      "video 1/1 (frame 282/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 283/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 138.0ms\n",
      "video 1/1 (frame 284/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 142.0ms\n",
      "video 1/1 (frame 285/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 286/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 287/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 288/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.6ms\n",
      "video 1/1 (frame 289/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 150.0ms\n",
      "video 1/1 (frame 290/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.1ms\n",
      "video 1/1 (frame 291/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 292/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 293/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 294/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 295/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 296/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.3ms\n",
      "video 1/1 (frame 297/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 133.0ms\n",
      "video 1/1 (frame 298/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 142.0ms\n",
      "video 1/1 (frame 299/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 300/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 301/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 302/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 303/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 304/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 305/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 306/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 307/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.9ms\n",
      "video 1/1 (frame 308/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 309/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 310/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 311/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 312/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 313/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 314/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 315/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 133.0ms\n",
      "video 1/1 (frame 316/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 317/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 318/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 319/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 320/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 321/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 322/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 323/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 324/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 133.0ms\n",
      "video 1/1 (frame 325/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 138.0ms\n",
      "video 1/1 (frame 326/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 327/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 138.0ms\n",
      "video 1/1 (frame 328/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 329/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 330/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 151.0ms\n",
      "video 1/1 (frame 331/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 332/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 333/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 334/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 335/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 336/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 337/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 338/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "video 1/1 (frame 339/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 340/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 341/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 342/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 343/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 344/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 134.1ms\n",
      "video 1/1 (frame 345/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.2ms\n",
      "video 1/1 (frame 346/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.1ms\n",
      "video 1/1 (frame 347/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 348/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.8ms\n",
      "video 1/1 (frame 349/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 350/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 140.0ms\n",
      "video 1/1 (frame 351/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 352/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 353/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 kite, 134.0ms\n",
      "video 1/1 (frame 354/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 355/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 356/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 357/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 358/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 359/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 360/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 140.0ms\n",
      "video 1/1 (frame 361/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 362/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 363/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 364/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 140.0ms\n",
      "video 1/1 (frame 365/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 366/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 140.0ms\n",
      "video 1/1 (frame 367/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 368/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 369/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 370/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 371/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 372/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 373/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 374/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.5ms\n",
      "video 1/1 (frame 375/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 376/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.1ms\n",
      "video 1/1 (frame 377/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 378/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 379/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 380/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 381/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 144.2ms\n",
      "video 1/1 (frame 382/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 140.9ms\n",
      "video 1/1 (frame 383/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 142.1ms\n",
      "video 1/1 (frame 384/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 188.0ms\n",
      "video 1/1 (frame 385/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 191.0ms\n",
      "video 1/1 (frame 386/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 147.0ms\n",
      "video 1/1 (frame 387/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 143.0ms\n",
      "video 1/1 (frame 388/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 140.0ms\n",
      "video 1/1 (frame 389/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 146.0ms\n",
      "video 1/1 (frame 390/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 159.0ms\n",
      "video 1/1 (frame 391/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 142.0ms\n",
      "video 1/1 (frame 392/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 393/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 394/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 395/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 141.0ms\n",
      "video 1/1 (frame 396/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 397/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 141.0ms\n",
      "video 1/1 (frame 398/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 399/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 400/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 401/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.7ms\n",
      "video 1/1 (frame 402/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 403/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 145.0ms\n",
      "video 1/1 (frame 404/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.2ms\n",
      "video 1/1 (frame 405/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 406/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 407/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 408/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 409/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 410/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 411/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 412/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.3ms\n",
      "video 1/1 (frame 413/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.2ms\n",
      "video 1/1 (frame 414/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.1ms\n",
      "video 1/1 (frame 415/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.1ms\n",
      "video 1/1 (frame 416/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.1ms\n",
      "video 1/1 (frame 417/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.2ms\n",
      "video 1/1 (frame 418/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 141.6ms\n",
      "video 1/1 (frame 419/419) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "Speed: 1.3ms preprocess, 136.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "model(source=\"data/output.mp4\", show=True, conf=0.5)\n",
    "\n",
    "# Param:\n",
    "# - source: Video Datei.\n",
    "# - show:   Öffne Fenster und zeige Video. \n",
    "# - conf:   Ab wie viel % eine Klasse als positive gewertet wird. \n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe8a897-0b34-44e3-bcde-ee294bb5801b",
   "metadata": {},
   "source": [
    "Hier machen wir dasselbe, diese Mal wird das Ergebnis auch als Video gespeichert. \n",
    "- Hier kann eine Abbruchbedingung eingefügt werde, Taste, Zeit, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3243412e-6e61-4d39-8ef6-0d7db8c5ddc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 1 kite, 140.0ms\n",
      "Speed: 3.0ms preprocess, 140.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 139.1ms\n",
      "Speed: 2.0ms preprocess, 139.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 139.6ms\n",
      "Speed: 1.0ms preprocess, 139.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 138.0ms\n",
      "Speed: 1.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 141.0ms\n",
      "Speed: 1.0ms preprocess, 141.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 139.0ms\n",
      "Speed: 2.0ms preprocess, 139.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 140.0ms\n",
      "Speed: 2.0ms preprocess, 140.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 140.0ms\n",
      "Speed: 1.0ms preprocess, 140.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 134.5ms\n",
      "Speed: 2.0ms preprocess, 134.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.1ms\n",
      "Speed: 2.0ms preprocess, 136.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 135.3ms\n",
      "Speed: 1.0ms preprocess, 135.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 141.8ms\n",
      "Speed: 2.0ms preprocess, 141.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.9ms\n",
      "Speed: 2.0ms preprocess, 134.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.1ms\n",
      "Speed: 1.0ms preprocess, 136.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 139.0ms\n",
      "Speed: 1.0ms preprocess, 139.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 140.0ms\n",
      "Speed: 1.0ms preprocess, 140.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.6ms\n",
      "Speed: 2.0ms preprocess, 135.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 143.0ms\n",
      "Speed: 1.0ms preprocess, 143.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 133.9ms\n",
      "Speed: 1.0ms preprocess, 133.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.1ms\n",
      "Speed: 2.0ms preprocess, 134.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.9ms\n",
      "Speed: 2.0ms preprocess, 134.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.6ms\n",
      "Speed: 1.0ms preprocess, 135.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.3ms\n",
      "Speed: 2.0ms preprocess, 134.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.1ms\n",
      "Speed: 2.0ms preprocess, 135.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.8ms\n",
      "Speed: 1.0ms preprocess, 134.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 138.1ms\n",
      "Speed: 2.0ms preprocess, 138.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.1ms\n",
      "Speed: 2.0ms preprocess, 135.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.3ms\n",
      "Speed: 2.0ms preprocess, 137.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.1ms\n",
      "Speed: 1.0ms preprocess, 134.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 141.0ms\n",
      "Speed: 1.0ms preprocess, 141.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 138.0ms\n",
      "Speed: 1.0ms preprocess, 138.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 141.0ms\n",
      "Speed: 1.0ms preprocess, 141.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 2 cell phones, 135.2ms\n",
      "Speed: 1.0ms preprocess, 135.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 134.1ms\n",
      "Speed: 1.0ms preprocess, 134.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 139.0ms\n",
      "Speed: 1.0ms preprocess, 139.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.8ms\n",
      "Speed: 1.0ms preprocess, 134.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.3ms\n",
      "Speed: 2.0ms preprocess, 136.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 144.4ms\n",
      "Speed: 2.0ms preprocess, 144.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 146.5ms\n",
      "Speed: 1.0ms preprocess, 146.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 2 cell phones, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 135.1ms\n",
      "Speed: 2.0ms preprocess, 135.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 133.7ms\n",
      "Speed: 2.0ms preprocess, 133.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 133.6ms\n",
      "Speed: 1.0ms preprocess, 133.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 2 cell phones, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 2 cell phones, 135.3ms\n",
      "Speed: 1.0ms preprocess, 135.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.8ms\n",
      "Speed: 1.0ms preprocess, 134.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.9ms\n",
      "Speed: 2.0ms preprocess, 135.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 142.0ms\n",
      "Speed: 1.0ms preprocess, 142.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.9ms\n",
      "Speed: 2.0ms preprocess, 135.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 142.0ms\n",
      "Speed: 1.0ms preprocess, 142.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "Speed: 2.1ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 135.1ms\n",
      "Speed: 2.0ms preprocess, 135.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 1 cell phone, 138.0ms\n",
      "Speed: 1.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 136.1ms\n",
      "Speed: 1.0ms preprocess, 136.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.4ms\n",
      "Speed: 2.0ms preprocess, 135.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 143.3ms\n",
      "Speed: 1.0ms preprocess, 143.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.2ms\n",
      "Speed: 1.0ms preprocess, 135.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.8ms\n",
      "Speed: 1.0ms preprocess, 136.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.1ms\n",
      "Speed: 2.0ms preprocess, 134.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.9ms\n",
      "Speed: 2.0ms preprocess, 134.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 135.1ms\n",
      "Speed: 1.0ms preprocess, 135.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 2 cell phones, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 144.0ms\n",
      "Speed: 2.0ms preprocess, 144.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 133.7ms\n",
      "Speed: 2.0ms preprocess, 133.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.1ms\n",
      "Speed: 1.0ms preprocess, 136.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 143.0ms\n",
      "Speed: 2.0ms preprocess, 143.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 139.0ms\n",
      "Speed: 2.0ms preprocess, 139.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 1 cell phone, 138.0ms\n",
      "Speed: 1.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 1 cell phone, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 kite, 137.4ms\n",
      "Speed: 1.0ms preprocess, 137.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 kite, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 kite, 1 cell phone, 136.8ms\n",
      "Speed: 2.0ms preprocess, 136.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 138.0ms\n",
      "Speed: 1.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 141.0ms\n",
      "Speed: 1.0ms preprocess, 141.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 kite, 1 cell phone, 134.3ms\n",
      "Speed: 2.0ms preprocess, 134.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 139.1ms\n",
      "Speed: 1.0ms preprocess, 139.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 139.7ms\n",
      "Speed: 1.0ms preprocess, 139.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 1 cell phone, 134.1ms\n",
      "Speed: 2.0ms preprocess, 134.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 140.0ms\n",
      "Speed: 1.0ms preprocess, 140.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 139.0ms\n",
      "Speed: 1.0ms preprocess, 139.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 139.0ms\n",
      "Speed: 2.0ms preprocess, 139.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 2 cell phones, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 2 cell phones, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3 cell phones, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 140.0ms\n",
      "Speed: 2.0ms preprocess, 140.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 133.9ms\n",
      "Speed: 2.0ms preprocess, 133.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.1ms\n",
      "Speed: 2.0ms preprocess, 135.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.4ms\n",
      "Speed: 1.0ms preprocess, 134.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 142.0ms\n",
      "Speed: 2.0ms preprocess, 142.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 144.0ms\n",
      "Speed: 2.0ms preprocess, 144.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.6ms\n",
      "Speed: 1.0ms preprocess, 135.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 tennis racket, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 2 cell phones, 145.0ms\n",
      "Speed: 1.0ms preprocess, 145.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 141.0ms\n",
      "Speed: 1.0ms preprocess, 141.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 141.0ms\n",
      "Speed: 1.0ms preprocess, 141.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 139.0ms\n",
      "Speed: 2.0ms preprocess, 139.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.1ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 2 cell phones, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 2 cell phones, 139.0ms\n",
      "Speed: 1.0ms preprocess, 139.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 3 cell phones, 140.0ms\n",
      "Speed: 2.0ms preprocess, 140.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 140.0ms\n",
      "Speed: 1.0ms preprocess, 140.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 141.0ms\n",
      "Speed: 2.0ms preprocess, 141.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 138.0ms\n",
      "Speed: 1.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 2 cell phones, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 2 cell phones, 139.0ms\n",
      "Speed: 1.0ms preprocess, 139.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 147.0ms\n",
      "Speed: 2.0ms preprocess, 147.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 kites, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 kites, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 kites, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 139.0ms\n",
      "Speed: 1.0ms preprocess, 139.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 139.0ms\n",
      "Speed: 2.0ms preprocess, 139.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 kite, 1 cell phone, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 141.0ms\n",
      "Speed: 1.0ms preprocess, 141.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 kites, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 142.0ms\n",
      "Speed: 1.0ms preprocess, 142.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 142.0ms\n",
      "Speed: 2.0ms preprocess, 142.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 138.0ms\n",
      "Speed: 1.0ms preprocess, 138.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 139.0ms\n",
      "Speed: 1.0ms preprocess, 139.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 139.0ms\n",
      "Speed: 2.0ms preprocess, 139.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 142.0ms\n",
      "Speed: 1.0ms preprocess, 142.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 148.0ms\n",
      "Speed: 2.0ms preprocess, 148.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 139.0ms\n",
      "Speed: 1.0ms preprocess, 139.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 139.0ms\n",
      "Speed: 1.0ms preprocess, 139.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 141.0ms\n",
      "Speed: 1.0ms preprocess, 141.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 144.0ms\n",
      "Speed: 1.0ms preprocess, 144.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "# # Lade Video und speichere Ergebnis # # \n",
    "\n",
    "video_input  = './data/output.mp4'\n",
    "video_output = './data/video_inference.mp4'\n",
    "vid          = cv.VideoCapture(video_path)\n",
    "resolution   = (640, 480)\n",
    "fps          = 27.0\n",
    "\n",
    "# Erstelle VideoWriter Objekt. \n",
    "fourcc = cv.VideoWriter_fourcc(*'H264')  \n",
    "out    = cv.VideoWriter(video_output, fourcc, fps, resolution)   \n",
    "\n",
    "\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "    if ret:\n",
    "        # Model Prediction:\n",
    "        prediction_on_image = model(frame, show=False)  # show: wenn True, werden die Frames vor dem Speichern angezeigt. \n",
    "        annotated_frame     = prediction_on_image[0].plot()\n",
    "        # Schreibe Videodatei. \n",
    "        out.write(annotated_frame)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "vid.release()\n",
    "out.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d78f1b-7dac-4d5a-8d37-efc040925bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Lade Video und zeige Prediction # # \n",
    "\n",
    "Video(\"data/video_inference.mp4\", embed=True, width=300, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575b762-df92-4c16-85ce-fb678609c40a",
   "metadata": {},
   "source": [
    "<h2>Object Detection live mit Kamera</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e521fe-dbd0-4224-b9f9-a8ce9d10cb0e",
   "metadata": {},
   "source": [
    "Dank Ultralystics ist das einsetzen einer Kamera für Real-Time Objetc Detecion sehr einfach (hier <u>ohne</u> Tracking). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec2df059-88a0-4e78-bb64-79b424ee34f4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_device(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_device' is not defined"
     ]
    }
   ],
   "source": [
    "test_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c120cb5-1300-42d5-933b-ee5a512b52a6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/1: 0... Success  (inf frames of shape 640x480 at 30.00 FPS)\n",
      "\n",
      "\n",
      "WARNING  inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "0: 480x640 1 person, 144.0ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 1 person, 141.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 141.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 144.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "0: 480x640 1 person, 1 kite, 136.0ms\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "0: 480x640 2 persons, 135.1ms\n",
      "0: 480x640 1 person, 1 kite, 134.6ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "0: 480x640 1 person, 1 kite, 137.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 2 persons, 1 kite, 136.0ms\n",
      "0: 480x640 2 persons, 1 kite, 138.0ms\n",
      "0: 480x640 2 persons, 1 kite, 136.0ms\n",
      "0: 480x640 2 persons, 1 kite, 135.0ms\n",
      "0: 480x640 2 persons, 1 kite, 136.0ms\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 142.0ms\n",
      "0: 480x640 1 person, 1 kite, 137.0ms\n",
      "0: 480x640 1 person, 1 kite, 137.0ms\n",
      "0: 480x640 1 person, 140.0ms\n",
      "0: 480x640 2 persons, 138.0ms\n",
      "0: 480x640 1 person, 1 kite, 144.0ms\n",
      "0: 480x640 1 person, 1 kite, 140.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.1ms\n",
      "0: 480x640 2 persons, 1 kite, 135.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 136.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 135.3ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 139.0ms\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.9ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 134.9ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 134.5ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 139.1ms\n",
      "0: 480x640 1 person, 140.0ms\n",
      "0: 480x640 1 person, 134.4ms\n",
      "0: 480x640 1 person, 134.6ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.5ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 140.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 139.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 145.4ms\n",
      "0: 480x640 1 person, 141.0ms\n",
      "0: 480x640 1 person, 142.0ms\n",
      "0: 480x640 1 person, 139.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 1 cell phone, 138.0ms\n",
      "0: 480x640 1 person, 143.0ms\n",
      "0: 480x640 1 person, 1 cell phone, 139.6ms\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "0: 480x640 1 person, 1 cell phone, 135.1ms\n",
      "0: 480x640 1 person, 134.3ms\n",
      "0: 480x640 1 person, 1 cell phone, 149.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "0: 480x640 2 persons, 135.1ms\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "0: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "0: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "0: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "0: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "0: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "0: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "0: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "0: 480x640 1 person, 1 cell phone, 139.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 141.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "0: 480x640 2 persons, 1 cell phone, 135.2ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 2 persons, 1 cell phone, 135.0ms\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "0: 480x640 2 persons, 1 cell phone, 136.0ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 2 persons, 1 cell phone, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 140.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.2ms\n",
      "0: 480x640 1 person, 140.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 1 person, 134.4ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 136.8ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "0: 480x640 1 person, 134.6ms\n",
      "0: 480x640 1 person, 133.8ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 139.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "0: 480x640 2 persons, 138.0ms\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 139.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "0: 480x640 1 person, 139.0ms\n",
      "0: 480x640 2 persons, 137.0ms\n",
      "0: 480x640 2 persons, 137.0ms\n",
      "0: 480x640 3 persons, 139.0ms\n",
      "0: 480x640 3 persons, 140.0ms\n",
      "0: 480x640 2 persons, 1 apple, 137.0ms\n",
      "0: 480x640 2 persons, 1 apple, 138.0ms\n",
      "0: 480x640 2 persons, 1 apple, 138.0ms\n",
      "0: 480x640 2 persons, 1 apple, 134.0ms\n",
      "0: 480x640 2 persons, 1 apple, 135.0ms\n",
      "0: 480x640 2 persons, 1 apple, 134.0ms\n",
      "0: 480x640 2 persons, 1 apple, 135.0ms\n",
      "0: 480x640 2 persons, 1 apple, 138.0ms\n",
      "0: 480x640 2 persons, 1 apple, 136.0ms\n",
      "0: 480x640 1 person, 1 apple, 138.0ms\n",
      "0: 480x640 2 persons, 1 apple, 134.0ms\n",
      "0: 480x640 1 person, 1 apple, 135.0ms\n",
      "0: 480x640 2 persons, 1 apple, 134.0ms\n",
      "0: 480x640 2 persons, 1 apple, 136.0ms\n",
      "0: 480x640 1 person, 1 apple, 135.0ms\n",
      "0: 480x640 2 persons, 1 apple, 136.0ms\n",
      "0: 480x640 1 person, 1 apple, 134.0ms\n",
      "0: 480x640 1 person, 1 apple, 136.0ms\n",
      "0: 480x640 1 person, 1 apple, 136.0ms\n",
      "0: 480x640 1 person, 1 apple, 134.0ms\n",
      "0: 480x640 1 person, 1 apple, 135.0ms\n",
      "0: 480x640 1 person, 134.2ms\n",
      "0: 480x640 2 persons, 139.4ms\n",
      "0: 480x640 2 persons, 134.1ms\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "0: 480x640 1 person, 134.8ms\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "0: 480x640 2 persons, 133.0ms\n",
      "0: 480x640 2 persons, 137.0ms\n",
      "0: 480x640 2 persons, 134.2ms\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "0: 480x640 2 persons, 135.2ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 1 apple, 136.0ms\n",
      "0: 480x640 1 person, 1 apple, 135.0ms\n",
      "0: 480x640 1 person, 1 apple, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 1 apple, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 1 apple, 139.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 1 apple, 134.0ms\n",
      "0: 480x640 1 person, 134.5ms\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.5ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "0: 480x640 2 persons, 138.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 2 persons, 134.1ms\n",
      "0: 480x640 1 person, 134.9ms\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "0: 480x640 1 person, 146.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 139.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 135.5ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.6ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 152.1ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.7ms\n",
      "0: 480x640 1 person, 133.5ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 142.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 1 person, 134.9ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 139.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 141.0ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 136.0ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, conf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\engine\\model.py:174\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    153\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    154\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    An alias for the predict method, enabling the model instance to be callable.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m        (List[ultralytics.engine.results.Results]): A list of prediction results, encapsulated in the Results class.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\engine\\model.py:442\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\_contextlib.py:56\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 56\u001b[0m                 response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:254\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 254\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:142\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    137\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    138\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    141\u001b[0m )\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:456\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 456\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39membed)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:102\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:120\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_once(x, profile, visualize, embed)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:141\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 141\u001b[0m x \u001b[38;5;241m=\u001b[39m m(x)  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    142\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\nn\\modules\\head.py:57\u001b[0m, in \u001b[0;36mDetect.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_end2end(x)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnl):\n\u001b[1;32m---> 57\u001b[0m     x[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2[i](x[i]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv3[i](x[i])), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:  \u001b[38;5;66;03m# Training path\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model(source=0, show=True, conf=0.5, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d8d7645-3eef-4411-9067-7a24097efc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.release()\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
