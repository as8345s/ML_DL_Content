{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b38b4024-8398-4518-bd1c-3a41365f8b64",
   "metadata": {},
   "source": [
    "<h1>YOLO</h1>\n",
    "[Details zur Funktionsweise von YOLO im anderen Notebook]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19920145-2279-4de0-923f-6095f1f0cd0e",
   "metadata": {},
   "source": [
    "<h2>1. Install</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca91060-a4a1-4408-8e9d-62be9bf53b1e",
   "metadata": {},
   "source": [
    "Um nicht alles per Hand zu programmieren, gibt es Libraries, die eine bequeme Nutzung von YOLO erlauben.\n",
    "\n",
    "Als Erstes muss PyTorch installiert werden. <br>\n",
    "> https://pytorch.org/get-started/locally/ [Letzter Zugriff: 08.07.2024]\n",
    "\n",
    "Folge diesen Anweisungen, um Ultralytics zu installieren: <br>\n",
    "> https://github.com/ultralytics/ultralytics [Letzter Zugriff: 08.07.2024] <br>\n",
    "> https://docs.ultralytics.com/quickstart/   [Letzter Zugriff: 08.07.2024]\n",
    "\n",
    "\n",
    "Danach kann dieser Befehl in die Konsole eingegeben werden: <br>\n",
    "`yolo predict model=yolov8s.pt source='https://ultralytics.com/images/bus.jpg'` <br>\n",
    "\n",
    "Damit wird ein Bild heruntergeladen und direkt für die Predicion genutzt. Dabei wird ein Pre-Trained Model verwendet.\n",
    "\n",
    "<u>Hinweis</u>:<br>\n",
    "Um diese Prediction auszuführen, müssen alle Libraries verfügbar sein. <br>\n",
    "Nicht vorhandene Libraries können z. B. mit `pip install <package>==<version>` installiert werden. <br>\n",
    "- Bei Numpy kann der folgende Fehler auftreten: `ImportError: numpy.core.multiarray failed to import`. Um das zu beheben muss eine spezifische Numpy Version installiert werden (hier: 1.26.0 statt 2.0.0).\n",
    "- Dasselbe gilt auf für andere Packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd87fb80-6281-4e97-aa23-d6c5fa886f1b",
   "metadata": {},
   "source": [
    "<h2>Predict</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d17ac0-a013-43a3-ad93-76a3d23abd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle imports.\n",
    "from ultralytics import YOLO\n",
    "import cv2 as cv \n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a718025a-df98-4f74-a4f2-6de2e3fd5f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.51 ðŸš€ Python-3.12.4 torch-2.3.1+cpu CPU (Intel Core(TM) i9-9900K 3.60GHz)\n",
      "YOLOv8s summary (fused): 168 layers, 11156544 parameters, 0 gradients, 28.6 GFLOPs\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\bus.jpg: 640x480 4 persons, 1 bus, 345.8ms\n",
      "Speed: 6.0ms preprocess, 345.8ms inference, 11.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mruns\\detect\\predict3\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "# Befehl kann auch direkt im Notebook ausgeführt werden.\n",
    "!yolo predict model=yolov8s.pt source='https://ultralytics.com/images/bus.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd34f1-d68a-420d-9f70-cd18facf286f",
   "metadata": {},
   "source": [
    "Das Ergebnis: <br>\n",
    "\n",
    "Linkes Bild: Iput <br>\n",
    "Rechtes Bild: Output<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc528ef-deb1-4cc8-9a54-7e0319efbcb8",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "      <td>\n",
    "      <img src=\"bus.jpg\" hight=300 width=300>\n",
    "      </td>\n",
    "      <td>\n",
    "        <img src=\"./runs/detect/predict/bus.jpg\"  hight=300 width=300>\n",
    "      </td>\n",
    "     </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7f4f5-aeb5-4144-b187-ae2eb4ed344e",
   "metadata": {},
   "source": [
    "Wie geht das?: <br>\n",
    "Das Netzwerk gibt ein Vektor aus, wo unter anderem die Daten für die Boundaries stehen. Diese Müssen nach Klassen in das Bild eingezeichnet werden. <br>\n",
    "Vor dem Einzeichnen wird ein Algorithmus names <u>IOU</u> verwendet, um überlappenden Boxen zu einer zu verschmelzen.\n",
    "- Zudem werden noch andere Daten ausgegeben, wie: 4 persons, 1 bus, ..., siehe unten.\n",
    "\n",
    "Das alles passiert intern, und als Ausgabe bekommen wir das Bild. Die eigentliche Ausgabe des Netzes ist ein Vektor, bestehend aus mehreren sogenannten <u>Anchor Boxen<u>. <br>\n",
    "\n",
    "Siehe auch: <br>\n",
    "> YOLOv3 From Scratch Using PyTorch: <br>\n",
    "> https://www.geeksforgeeks.org/yolov3-from-scratch-using-pytorch/ [Letzter Zugriff: 08.07.2024]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2648d6b6-954b-49bb-b557-11e897b3240b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 151.0ms\n",
      "Speed: 3.0ms preprocess, 151.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "## Als Code: ## \n",
    "\n",
    "# Lade Model Typ Nano. \n",
    "# - 8n: YOLO Nano, es gibt noch s: small, L: Large und x: extra large.\n",
    "# - Pre-Trained mit COCO Dataset.\n",
    "model = YOLO(\"yolov8n.pt\")  \n",
    "\n",
    "# Führe Prediction aus. \n",
    "results = model(\"bus.jpg\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4421d3b-bfb4-4737-9540-dc7851d272d6",
   "metadata": {},
   "source": [
    "<h2>OpenCV Kamera</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beecbacc-e046-46dd-88d9-476d49d624e0",
   "metadata": {},
   "source": [
    "Wir wollen ja nicht nur einzelne Bilder für die Predictions nutzen, sonders auch Videos und Livestreams.\n",
    "\n",
    "Für diesen Aufbau nutzen wir eine einfache USB Kamera, die über den PC angeschlossen wird. <br>\n",
    "Dank OpenCV können wir diese ganz einfach nutzen.\n",
    "\n",
    "Siehe auch: <br>\n",
    "https://docs.opencv.org/3.4/dd/d43/tutorial_py_video_display.html [Letzter Zugriff: 08.07.2024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1250e235-528c-4b2e-a621-91db63917f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Keine Kamera bei der Quelle: 1\n"
     ]
    }
   ],
   "source": [
    "# #  Kamera da? # # \n",
    "\n",
    "# Teste Quelle.\n",
    "import cv2 as cv \n",
    "\n",
    "def test_device(source):\n",
    "   cap = cv.VideoCapture(source)  # Source: Standardmäßig ist 0 die Kamera. \n",
    "   if cap is None or not cap.isOpened():\n",
    "       print(f'Warning: Keine Kamera bei der Quelle: {source}')\n",
    "\n",
    "test_device(0)\n",
    "test_device(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "920734d4-0943-452e-8d11-a12e1affcafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Öffne Videokanal und Filme # # \n",
    "\n",
    "# Öffne Videokanal. \n",
    "vid = cv.VideoCapture(0) \n",
    "\n",
    "if not vid.isOpened():\n",
    " print(\"Kamera nicht verfügbar\")\n",
    " exit()\n",
    "  \n",
    "while(True): \n",
    "      \n",
    "    # Erfasse Video:\n",
    "    ret, frame = vid.read() \n",
    "  \n",
    "    # Öffne Fenster und zeige Frame. \n",
    "    cv.imshow('frame', frame) \n",
    "\n",
    "    # Durch Taste 'q' soll der Stream beendet werden. \n",
    "    if cv.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "  \n",
    "# Freigeben der Kamera. \n",
    "vid.release() \n",
    "# Schließe alle Fenster. \n",
    "cv.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad306579-f46e-4ec7-aa30-d68fac118c41",
   "metadata": {},
   "source": [
    "Jetzt wollen wir ein Video mit der Kamera aufnehmen und speichern. Danach soll das gespeicherte Video geladen und für die Prediction genutzt werden.\n",
    "\n",
    "Dabei stellen wir ein, dass die Aufnahme n-Sekunden gehen soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7abf8779-af4b-417b-a3c1-173e717def58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Nehme Video auf und speichere # # \n",
    "\n",
    "time_limit  = 15\n",
    "fps         = 27.0\n",
    "resolution  = (640, 480)\n",
    "video_path  = './data/output.mp4'\n",
    "\n",
    "vid = cv.VideoCapture(0)\n",
    "if not vid.isOpened():\n",
    "   print(\"Kamera nicht verfügbar\")\n",
    "   exit()\n",
    "    \n",
    "# Erstelle VideoWriter Objekt. \n",
    "fourcc = cv.VideoWriter_fourcc(*'mp4v')  #  mp4v(.mp4, )XVID(.avi), MJPG(.mp4) Weitere Formate auf der OpenCV Webseite. \n",
    "out = cv.VideoWriter(video_path, fourcc, fps, resolution)   # VideoWriter(Datei, fourcc, FPS, res( , ) )\n",
    "\n",
    "# Starte Timer.\n",
    "start_time = time.time()\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "\n",
    "    if ret:\n",
    "        out.write(frame)\n",
    "        # Öffne Fenster und zeige Frame.\n",
    "        cv.imshow('frame', frame)\n",
    "        # Limitieren Aufnahmezeit. \n",
    "        if time.time() - start_time > time_limit:\n",
    "            break\n",
    "        # Drücke Taste zum aufhören. \n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "vid.release()\n",
    "out.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ffcee-ff8a-44e1-aad9-cce1cf8dffa3",
   "metadata": {},
   "source": [
    "Wir können testet, ob diese Datei geschrieben wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed4885b-0567-4d13-b951-da79883c2b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bbb602f-d6d3-4ca3-9cb3-bcdc5ae25bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Größe:    3764140 bytes\n",
      "Erstellt: Wed Jul 10 13:29:19 2024\n"
     ]
    }
   ],
   "source": [
    "print(f\"Größe:    {os.stat(video_path).st_size} bytes\")\n",
    "print(f\"Erstellt: {time.ctime(os.stat(video_path).st_ctime)}\")\n",
    "# ggf. weitere Informationen ausgeben... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8e06c-7c3e-48dd-84b4-a776757a7c09",
   "metadata": {},
   "source": [
    "Jetzt kann das Video geladen und abgespielt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24384181-1113-4f74-ade5-f42fcf137b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keine Frames mehr erhalten\n"
     ]
    }
   ],
   "source": [
    "# # Playback des aufgenommenen Videos # # \n",
    "\n",
    "cap = cv.VideoCapture(video_path)\n",
    "\n",
    "while cap.isOpened():\n",
    "     ret, frame = cap.read()\n",
    "    \n",
    "     if not ret:\n",
    "         print(\"Keine Frames mehr erhalten\")\n",
    "         break\n",
    "     # Zeige in einem Fenster das aufgenommene Video.\n",
    "     cv.imshow('frame', frame)\n",
    "     if cv.waitKey(1) == ord('q'):\n",
    "         break\n",
    "         \n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac70a6-0263-47c3-ae13-5a5e17d4a980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21054f21-40f3-4bbe-90d0-63a58d651f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# // Issue with displaying videos, will be fixed soon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba517672-ad3e-4361-9497-4641577e072f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33c6df76-5856-4ef6-97d8-583e0c1f5b2d",
   "metadata": {},
   "source": [
    "Alternativ können wir auch Videos hier in dem Notebook einbinden, was ganz nett sein kann. Mit weiteren CSS und HTML Elementen könnte man das weiter ausbauen...\n",
    "- In Jupyter Notebooks können CSS und HTML Elemente genutzt werden, um Inhalte benutzerdefiniert darzustellen.\n",
    "\n",
    "<u>Hinweis</u>:<br>\n",
    "Damit das Video richtig angezeigt wird, muss es ggf. anders codiert werden.: <br>\n",
    "`ffmpeg -i data/output.mp4 -vcodec h264 -acodec mp2 data/output_enc.mp4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f14edf44-9bb3-4753-96de-378bce74a233",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -i data/output.mp4 -vcodec h264 -acodec mp2 data/output_enc.mp4"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f43e925e-7830-41ad-b179-a715fee487cd",
   "metadata": {},
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"data/output_enc.mp4\", embed=True, width=300, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055276b-5d92-453a-9f35-dea9fe8c65e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f07d6cdc-744f-44de-9690-05b1f6bf93d7",
   "metadata": {},
   "source": [
    "<h2>Object Detection mit gespeicherten Videos</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6da20e-3fda-4353-b2df-5d8301c825ae",
   "metadata": {},
   "source": [
    "Mit dem angegebenen Model kann das Video, was wir erstellt haben, jetzt die Inference verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bc920dc-9df9-42b2-9655-344f61b1f0a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "WARNING  inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "video 1/1 (frame 1/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 147.5ms\n",
      "video 1/1 (frame 2/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 146.1ms\n",
      "video 1/1 (frame 3/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 140.2ms\n",
      "video 1/1 (frame 4/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 138.7ms\n",
      "video 1/1 (frame 5/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 6/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 139.0ms\n",
      "video 1/1 (frame 7/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 8/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 137.0ms\n",
      "video 1/1 (frame 9/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 10/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 139.0ms\n",
      "video 1/1 (frame 11/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 12/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 13/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 168.1ms\n",
      "video 1/1 (frame 14/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 15/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 16/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 17/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.4ms\n",
      "video 1/1 (frame 18/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 19/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 20/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 137.0ms\n",
      "video 1/1 (frame 21/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 22/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 23/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 140.0ms\n",
      "video 1/1 (frame 24/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 25/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 26/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.9ms\n",
      "video 1/1 (frame 27/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 28/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 29/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 30/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 31/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 32/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 33/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 137.0ms\n",
      "video 1/1 (frame 34/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 138.0ms\n",
      "video 1/1 (frame 35/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 36/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 37/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 38/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 39/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 40/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 138.0ms\n",
      "video 1/1 (frame 41/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.1ms\n",
      "video 1/1 (frame 42/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 137.0ms\n",
      "video 1/1 (frame 43/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 44/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 137.0ms\n",
      "video 1/1 (frame 45/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 137.0ms\n",
      "video 1/1 (frame 46/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 47/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 48/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 49/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 50/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 51/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.9ms\n",
      "video 1/1 (frame 52/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 137.6ms\n",
      "video 1/1 (frame 53/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 140.0ms\n",
      "video 1/1 (frame 54/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 150.0ms\n",
      "video 1/1 (frame 55/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 157.0ms\n",
      "video 1/1 (frame 56/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 139.0ms\n",
      "video 1/1 (frame 57/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 138.0ms\n",
      "video 1/1 (frame 58/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 141.0ms\n",
      "video 1/1 (frame 59/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 138.0ms\n",
      "video 1/1 (frame 60/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 61/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 62/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 63/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 64/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 156.0ms\n",
      "video 1/1 (frame 65/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 66/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 67/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 68/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 69/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 70/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 71/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 72/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 140.0ms\n",
      "video 1/1 (frame 73/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 74/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 75/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 137.0ms\n",
      "video 1/1 (frame 76/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 77/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 139.0ms\n",
      "video 1/1 (frame 78/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 142.0ms\n",
      "video 1/1 (frame 79/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 141.0ms\n",
      "video 1/1 (frame 80/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 81/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 82/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 142.0ms\n",
      "video 1/1 (frame 83/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 84/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 85/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 86/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 141.0ms\n",
      "video 1/1 (frame 87/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 88/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 89/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 90/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 91/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 92/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 93/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 94/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 95/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 96/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 97/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 98/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 99/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 100/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 137.0ms\n",
      "video 1/1 (frame 101/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 102/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 103/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 104/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 105/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 106/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 107/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 108/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 109/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 140.0ms\n",
      "video 1/1 (frame 110/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 142.0ms\n",
      "video 1/1 (frame 111/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 112/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 113/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 114/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 138.0ms\n",
      "video 1/1 (frame 115/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 138.0ms\n",
      "video 1/1 (frame 116/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 178.0ms\n",
      "video 1/1 (frame 117/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 118/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 119/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 120/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 121/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 122/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 137.0ms\n",
      "video 1/1 (frame 123/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 124/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 140.0ms\n",
      "video 1/1 (frame 125/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 126/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 127/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 128/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 129/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 130/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 131/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 132/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 133/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 134/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 135/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 136/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 137/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 138/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 139/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 140/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 141/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 142/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 143/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 144/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 145/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 146/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 147/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 148/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 149/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 143.0ms\n",
      "video 1/1 (frame 150/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 151/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 152/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 153/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 154/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 155/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 139.0ms\n",
      "video 1/1 (frame 156/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 138.0ms\n",
      "video 1/1 (frame 157/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 139.0ms\n",
      "video 1/1 (frame 158/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 140.0ms\n",
      "video 1/1 (frame 159/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 160/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 161/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 140.0ms\n",
      "video 1/1 (frame 162/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 139.0ms\n",
      "video 1/1 (frame 163/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 164/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 165/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 166/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 167/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 168/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 169/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 170/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 171/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 160.0ms\n",
      "video 1/1 (frame 172/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 173/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 174/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 164.0ms\n",
      "video 1/1 (frame 175/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 176/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 177/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 178/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 179/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 180/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 181/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 182/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 183/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 184/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 185/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 186/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 187/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 188/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 189/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 190/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 191/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 192/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 144.0ms\n",
      "video 1/1 (frame 193/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 194/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 195/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 196/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 197/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 198/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 199/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 200/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 201/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 202/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 203/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 137.0ms\n",
      "video 1/1 (frame 204/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 205/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 206/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 207/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 208/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 137.0ms\n",
      "video 1/1 (frame 209/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 210/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 137.0ms\n",
      "video 1/1 (frame 211/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 141.0ms\n",
      "video 1/1 (frame 212/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 213/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 214/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 remote, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 215/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 remote, 136.0ms\n",
      "video 1/1 (frame 216/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 remote, 138.0ms\n",
      "video 1/1 (frame 217/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 remote, 136.0ms\n",
      "video 1/1 (frame 218/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 remote, 136.0ms\n",
      "video 1/1 (frame 219/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 remote, 136.0ms\n",
      "video 1/1 (frame 220/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 remote, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 221/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 137.0ms\n",
      "video 1/1 (frame 222/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 remote, 136.0ms\n",
      "video 1/1 (frame 223/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 224/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 140.0ms\n",
      "video 1/1 (frame 225/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 226/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 153.0ms\n",
      "video 1/1 (frame 227/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 138.0ms\n",
      "video 1/1 (frame 228/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 229/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 145.0ms\n",
      "video 1/1 (frame 230/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 231/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 232/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 233/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 234/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 235/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 236/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 237/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 238/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 239/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 139.0ms\n",
      "video 1/1 (frame 240/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 150.0ms\n",
      "video 1/1 (frame 241/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 242/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 243/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 244/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 245/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 246/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 247/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 248/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 249/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 250/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 251/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 252/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 253/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 254/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 255/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 256/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 257/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 137.0ms\n",
      "video 1/1 (frame 258/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 259/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 260/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 261/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 144.0ms\n",
      "video 1/1 (frame 262/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 140.0ms\n",
      "video 1/1 (frame 263/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 141.0ms\n",
      "video 1/1 (frame 264/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 140.0ms\n",
      "video 1/1 (frame 265/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 144.0ms\n",
      "video 1/1 (frame 266/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 142.0ms\n",
      "video 1/1 (frame 267/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 268/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 269/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 270/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 271/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 272/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 155.0ms\n",
      "video 1/1 (frame 273/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 274/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 275/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 276/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 277/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 278/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 279/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 280/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 281/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 282/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 142.0ms\n",
      "video 1/1 (frame 283/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 141.0ms\n",
      "video 1/1 (frame 284/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 285/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 286/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 287/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 288/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 289/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 290/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 147.0ms\n",
      "video 1/1 (frame 291/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 292/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 140.0ms\n",
      "video 1/1 (frame 293/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 294/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 295/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 296/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 297/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 298/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 299/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 300/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 301/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 302/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 303/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 304/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 137.0ms\n",
      "video 1/1 (frame 305/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 306/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 307/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 308/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 140.0ms\n",
      "video 1/1 (frame 309/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 310/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 140.0ms\n",
      "video 1/1 (frame 311/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 312/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 138.0ms\n",
      "video 1/1 (frame 313/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 140.0ms\n",
      "video 1/1 (frame 314/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 155.0ms\n",
      "video 1/1 (frame 315/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 135.0ms\n",
      "video 1/1 (frame 316/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 317/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 318/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 319/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 320/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 321/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 322/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 323/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 324/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 325/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 326/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 327/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 137.0ms\n",
      "video 1/1 (frame 328/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 141.0ms\n",
      "video 1/1 (frame 329/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 330/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 141.0ms\n",
      "video 1/1 (frame 331/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 332/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 333/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 334/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 335/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 336/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 337/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 338/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 339/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 340/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 341/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 141.0ms\n",
      "video 1/1 (frame 342/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 343/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 344/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 345/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 346/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 347/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 134.0ms\n",
      "video 1/1 (frame 348/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 349/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 350/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 351/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 352/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 353/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 159.0ms\n",
      "video 1/1 (frame 354/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 355/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 356/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 357/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 358/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 154.0ms\n",
      "video 1/1 (frame 359/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 360/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 138.0ms\n",
      "video 1/1 (frame 361/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 362/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 363/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 364/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 143.0ms\n",
      "video 1/1 (frame 365/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 139.0ms\n",
      "video 1/1 (frame 366/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 138.0ms\n",
      "video 1/1 (frame 367/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 139.0ms\n",
      "video 1/1 (frame 368/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 139.0ms\n",
      "video 1/1 (frame 369/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 142.0ms\n",
      "video 1/1 (frame 370/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 371/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 372/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 373/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 374/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 375/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 376/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 (no detections), 136.0ms\n",
      "video 1/1 (frame 377/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 378/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 379/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 380/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 381/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 382/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 139.0ms\n",
      "video 1/1 (frame 383/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "video 1/1 (frame 384/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 385/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 386/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 139.0ms\n",
      "video 1/1 (frame 387/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 388/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 389/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 162.0ms\n",
      "video 1/1 (frame 390/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 391/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 392/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "video 1/1 (frame 393/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 141.0ms\n",
      "video 1/1 (frame 394/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "video 1/1 (frame 395/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 396/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 140.0ms\n",
      "video 1/1 (frame 397/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 398/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 399/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 400/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 143.0ms\n",
      "video 1/1 (frame 401/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 402/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 403/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 404/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 405/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 2 persons, 137.0ms\n",
      "video 1/1 (frame 406/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 138.0ms\n",
      "video 1/1 (frame 407/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 408/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 409/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 141.0ms\n",
      "video 1/1 (frame 410/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 411/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 412/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 413/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 414/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 415/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 1 cell phone, 138.0ms\n",
      "video 1/1 (frame 416/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "video 1/1 (frame 417/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 143.0ms\n",
      "video 1/1 (frame 418/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 419/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 137.0ms\n",
      "video 1/1 (frame 420/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 421/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 136.0ms\n",
      "video 1/1 (frame 422/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 165.0ms\n",
      "video 1/1 (frame 423/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 135.0ms\n",
      "video 1/1 (frame 424/424) C:\\Users\\Alexander\\Desktop\\ML_DL_self_learn\\GitHub\\ML_DL_Content\\deep_learning\\PyTorch\\projects\\1__YOLO\\data\\output.mp4: 480x640 1 person, 139.0ms\n",
      "Speed: 1.2ms preprocess, 137.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "model(source=\"data/output.mp4\", show=True, conf=0.5)\n",
    "\n",
    "# Param:\n",
    "# - source: Video Datei.\n",
    "# - show:   Öffne Fenster und zeige Video. \n",
    "# - conf:   Ab wie viel % eine Klasse als positive gewertet wird. \n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe8a897-0b34-44e3-bcde-ee294bb5801b",
   "metadata": {},
   "source": [
    "Hier machen wir dasselbe, diese Mal wird das Ergebnis auch als Video gespeichert. \n",
    "- Hier kann eine Abbruchbedingung eingefügt werde, Taste, Zeit, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3243412e-6e61-4d39-8ef6-0d7db8c5ddc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 140.0ms\n",
      "Speed: 2.0ms preprocess, 140.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.9ms\n",
      "Speed: 1.0ms preprocess, 136.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 152.0ms\n",
      "Speed: 1.0ms preprocess, 152.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 140.9ms\n",
      "Speed: 2.0ms preprocess, 140.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 159.0ms\n",
      "Speed: 2.0ms preprocess, 159.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 139.0ms\n",
      "Speed: 2.0ms preprocess, 139.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 158.0ms\n",
      "Speed: 2.0ms preprocess, 158.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 133.9ms\n",
      "Speed: 2.0ms preprocess, 133.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 147.3ms\n",
      "Speed: 1.0ms preprocess, 147.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 134.9ms\n",
      "Speed: 2.0ms preprocess, 134.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 134.7ms\n",
      "Speed: 2.0ms preprocess, 134.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 133.6ms\n",
      "Speed: 1.0ms preprocess, 133.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 140.0ms\n",
      "Speed: 2.0ms preprocess, 140.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 148.0ms\n",
      "Speed: 1.0ms preprocess, 148.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 149.0ms\n",
      "Speed: 2.0ms preprocess, 149.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 backpack, 133.9ms\n",
      "Speed: 1.0ms preprocess, 133.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 138.0ms\n",
      "Speed: 1.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 142.0ms\n",
      "Speed: 1.0ms preprocess, 142.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 149.0ms\n",
      "Speed: 2.0ms preprocess, 149.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.9ms\n",
      "Speed: 1.0ms preprocess, 134.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 141.0ms\n",
      "Speed: 2.0ms preprocess, 141.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 146.0ms\n",
      "Speed: 3.0ms preprocess, 146.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 139.0ms\n",
      "Speed: 1.0ms preprocess, 139.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 133.9ms\n",
      "Speed: 2.0ms preprocess, 133.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 133.9ms\n",
      "Speed: 2.0ms preprocess, 133.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 154.0ms\n",
      "Speed: 2.0ms preprocess, 154.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 142.2ms\n",
      "Speed: 1.0ms preprocess, 142.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.9ms\n",
      "Speed: 2.0ms preprocess, 134.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 133.9ms\n",
      "Speed: 2.0ms preprocess, 133.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 140.0ms\n",
      "Speed: 1.0ms preprocess, 140.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 156.0ms\n",
      "Speed: 2.0ms preprocess, 156.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 133.9ms\n",
      "Speed: 2.0ms preprocess, 133.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 141.0ms\n",
      "Speed: 2.0ms preprocess, 141.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 149.0ms\n",
      "Speed: 2.0ms preprocess, 149.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 teddy bear, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 teddy bear, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 teddy bear, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 teddy bear, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 teddy bear, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 teddy bear, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 1 teddy bear, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 teddy bear, 134.9ms\n",
      "Speed: 1.0ms preprocess, 134.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 150.0ms\n",
      "Speed: 1.0ms preprocess, 150.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 141.0ms\n",
      "Speed: 1.0ms preprocess, 141.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.9ms\n",
      "Speed: 1.0ms preprocess, 134.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 140.0ms\n",
      "Speed: 1.0ms preprocess, 140.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.9ms\n",
      "Speed: 1.0ms preprocess, 135.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 152.0ms\n",
      "Speed: 1.0ms preprocess, 152.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.6ms\n",
      "Speed: 2.0ms preprocess, 134.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 153.0ms\n",
      "Speed: 2.0ms preprocess, 153.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 147.0ms\n",
      "Speed: 2.0ms preprocess, 147.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 153.0ms\n",
      "Speed: 2.0ms preprocess, 153.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 133.0ms\n",
      "Speed: 1.0ms preprocess, 133.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 150.0ms\n",
      "Speed: 1.0ms preprocess, 150.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 142.0ms\n",
      "Speed: 2.0ms preprocess, 142.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 140.0ms\n",
      "Speed: 1.0ms preprocess, 140.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 164.0ms\n",
      "Speed: 1.0ms preprocess, 164.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 141.0ms\n",
      "Speed: 2.0ms preprocess, 141.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 155.0ms\n",
      "Speed: 2.0ms preprocess, 155.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 139.0ms\n",
      "Speed: 1.0ms preprocess, 139.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 143.0ms\n",
      "Speed: 2.0ms preprocess, 143.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.9ms\n",
      "Speed: 2.0ms preprocess, 135.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 191.0ms\n",
      "Speed: 4.0ms preprocess, 191.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 143.0ms\n",
      "Speed: 2.0ms preprocess, 143.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 142.0ms\n",
      "Speed: 2.0ms preprocess, 142.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 142.0ms\n",
      "Speed: 2.0ms preprocess, 142.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 143.0ms\n",
      "Speed: 1.0ms preprocess, 143.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 140.0ms\n",
      "Speed: 1.0ms preprocess, 140.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 162.0ms\n",
      "Speed: 2.0ms preprocess, 162.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 143.0ms\n",
      "Speed: 2.0ms preprocess, 143.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 140.0ms\n",
      "Speed: 1.0ms preprocess, 140.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 140.0ms\n",
      "Speed: 1.0ms preprocess, 140.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 140.0ms\n",
      "Speed: 2.0ms preprocess, 140.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 139.0ms\n",
      "Speed: 2.0ms preprocess, 139.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.9ms\n",
      "Speed: 2.0ms preprocess, 137.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 145.0ms\n",
      "Speed: 2.0ms preprocess, 145.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 142.0ms\n",
      "Speed: 2.0ms preprocess, 142.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 142.0ms\n",
      "Speed: 1.0ms preprocess, 142.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 143.0ms\n",
      "Speed: 3.0ms preprocess, 143.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 153.0ms\n",
      "Speed: 2.0ms preprocess, 153.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 152.0ms\n",
      "Speed: 2.0ms preprocess, 152.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 1 cell phone, 151.0ms\n",
      "Speed: 2.0ms preprocess, 151.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 kites, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 umbrella, 1 kite, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.9ms\n",
      "Speed: 2.0ms preprocess, 134.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 151.0ms\n",
      "Speed: 2.0ms preprocess, 151.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 remote, 2 cell phones, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3 cell phones, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 138.0ms\n",
      "Speed: 1.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.9ms\n",
      "Speed: 1.0ms preprocess, 134.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 139.0ms\n",
      "Speed: 1.0ms preprocess, 139.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 139.0ms\n",
      "Speed: 1.0ms preprocess, 139.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 154.0ms\n",
      "Speed: 2.0ms preprocess, 154.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 141.0ms\n",
      "Speed: 1.0ms preprocess, 141.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 133.0ms\n",
      "Speed: 2.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.9ms\n",
      "Speed: 1.0ms preprocess, 134.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 138.0ms\n",
      "Speed: 1.0ms preprocess, 138.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 149.0ms\n",
      "Speed: 2.0ms preprocess, 149.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 141.0ms\n",
      "Speed: 2.0ms preprocess, 141.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3 cell phones, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 141.0ms\n",
      "Speed: 1.0ms preprocess, 141.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.9ms\n",
      "Speed: 2.0ms preprocess, 135.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 2 cell phones, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 3 cell phones, 149.0ms\n",
      "Speed: 2.0ms preprocess, 149.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 133.9ms\n",
      "Speed: 2.0ms preprocess, 133.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 135.9ms\n",
      "Speed: 1.0ms preprocess, 135.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 133.0ms\n",
      "Speed: 1.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 1.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 139.0ms\n",
      "Speed: 1.0ms preprocess, 139.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 2 cell phones, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 2 cell phones, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 152.0ms\n",
      "Speed: 2.0ms preprocess, 152.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 2 cell phones, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 137.0ms\n",
      "Speed: 1.0ms preprocess, 137.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 133.0ms\n",
      "Speed: 1.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 1 cell phone, 135.0ms\n",
      "Speed: 1.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3 cell phones, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 135.0ms\n",
      "Speed: 2.0ms preprocess, 135.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "# # Lade Video und speichere Ergebnis # # \n",
    "\n",
    "video_input  = './data/output.mp4'\n",
    "video_output = './data/video_inference.mp4'\n",
    "vid          = cv.VideoCapture(video_path)\n",
    "resolution   = (640, 480)\n",
    "fps          = 27.0\n",
    "\n",
    "# Erstelle VideoWriter Objekt. \n",
    "fourcc = cv.VideoWriter_fourcc(*'mp4v')  \n",
    "out    = cv.VideoWriter(video_output, fourcc, fps, resolution)   \n",
    "\n",
    "\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "    if ret:\n",
    "        # Model Prediction:\n",
    "        prediction_on_image = model(frame)\n",
    "        annotated_frame     = prediction_on_image[0].plot()\n",
    "        # Schreibe Videodatei. \n",
    "        out.write(annotated_frame)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "vid.release()\n",
    "out.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575b762-df92-4c16-85ce-fb678609c40a",
   "metadata": {},
   "source": [
    "<h2>Object Detection live mit Kamera</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e521fe-dbd0-4224-b9f9-a8ce9d10cb0e",
   "metadata": {},
   "source": [
    "Dank Ultralystics ist das einsetzen einer Kamera für Real-Time Objetc Detecion sehr einfach (<u>ohne</u> Tracking). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec2df059-88a0-4e78-bb64-79b424ee34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c120cb5-1300-42d5-933b-ee5a512b52a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/1: 0... Success  (inf frames of shape 640x480 at 30.00 FPS)\n",
      "\n",
      "\n",
      "WARNING  inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "0: 480x640 (no detections), 154.0ms\n",
      "0: 480x640 1 person, 141.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 144.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 139.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 140.0ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 1 person, 137.0ms\n",
      "0: 480x640 1 person, 136.4ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.9ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 147.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 133.1ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.1ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 159.0ms\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 146.0ms\n",
      "0: 480x640 1 person, 1 kite, 133.0ms\n",
      "0: 480x640 1 person, 1 kite, 133.0ms\n",
      "0: 480x640 1 person, 1 kite, 133.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 1 kite, 136.1ms\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "0: 480x640 1 person, 1 kite, 140.0ms\n",
      "0: 480x640 1 person, 138.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 2 kites, 133.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 136.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 136.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 133.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 135.0ms\n",
      "0: 480x640 1 person, 1 kite, 154.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 140.0ms\n",
      "0: 480x640 3 persons, 1 kite, 135.0ms\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 137.0ms\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "0: 480x640 3 persons, 150.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 136.3ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 133.0ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 2 persons, 133.5ms\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 3 persons, 135.0ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 2 persons, 133.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 2 persons, 134.4ms\n",
      "0: 480x640 3 persons, 133.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 133.0ms\n",
      "0: 480x640 4 persons, 1 kite, 133.0ms\n",
      "0: 480x640 2 persons, 1 kite, 136.0ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 137.0ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 2 persons, 136.0ms\n",
      "0: 480x640 2 persons, 135.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 136.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.9ms\n",
      "0: 480x640 1 person, 1 kite, 133.0ms\n",
      "0: 480x640 1 person, 1 kite, 133.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.1ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "0: 480x640 2 persons, 1 kite, 137.0ms\n",
      "0: 480x640 2 persons, 1 kite, 135.0ms\n",
      "0: 480x640 2 persons, 1 kite, 137.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.3ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 137.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "0: 480x640 3 persons, 1 kite, 135.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 148.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "0: 480x640 2 persons, 1 kite, 138.0ms\n",
      "0: 480x640 2 persons, 1 kite, 138.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.3ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 135.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 135.0ms\n",
      "0: 480x640 2 persons, 1 kite, 137.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.9ms\n",
      "0: 480x640 2 persons, 1 kite, 136.0ms\n",
      "0: 480x640 2 persons, 1 kite, 136.0ms\n",
      "0: 480x640 3 persons, 1 kite, 136.0ms\n",
      "0: 480x640 3 persons, 1 kite, 137.0ms\n",
      "0: 480x640 2 persons, 1 kite, 137.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "0: 480x640 2 persons, 1 kite, 135.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.5ms\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 135.0ms\n",
      "0: 480x640 2 persons, 1 kite, 132.9ms\n",
      "0: 480x640 4 persons, 1 kite, 136.0ms\n",
      "0: 480x640 1 person, 1 kite, 133.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 4 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 136.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 138.0ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "0: 480x640 2 persons, 1 kite, 135.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 4 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 4 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 133.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 133.0ms\n",
      "0: 480x640 4 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 145.9ms\n",
      "0: 480x640 3 persons, 1 kite, 133.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 135.0ms\n",
      "0: 480x640 4 persons, 1 kite, 135.0ms\n",
      "0: 480x640 3 persons, 1 kite, 143.0ms\n",
      "0: 480x640 3 persons, 1 kite, 135.0ms\n",
      "0: 480x640 3 persons, 1 kite, 135.0ms\n",
      "0: 480x640 3 persons, 1 kite, 135.0ms\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 3 persons, 135.0ms\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "0: 480x640 3 persons, 133.0ms\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 135.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 135.0ms\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 135.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 148.0ms\n",
      "0: 480x640 3 persons, 1 kite, 153.0ms\n",
      "0: 480x640 3 persons, 1 kite, 135.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 137.0ms\n",
      "0: 480x640 3 persons, 2 kites, 134.0ms\n",
      "0: 480x640 3 persons, 2 kites, 139.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 2 kites, 136.0ms\n",
      "0: 480x640 3 persons, 2 kites, 134.0ms\n",
      "0: 480x640 3 persons, 2 kites, 133.3ms\n",
      "0: 480x640 3 persons, 135.0ms\n",
      "0: 480x640 3 persons, 2 kites, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 137.0ms\n",
      "0: 480x640 3 persons, 2 kites, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 135.0ms\n",
      "0: 480x640 3 persons, 135.0ms\n",
      "0: 480x640 3 persons, 1 kite, 135.0ms\n",
      "0: 480x640 3 persons, 135.0ms\n",
      "0: 480x640 3 persons, 135.0ms\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "0: 480x640 3 persons, 133.0ms\n",
      "0: 480x640 3 persons, 1 kite, 133.0ms\n",
      "0: 480x640 3 persons, 134.0ms\n",
      "0: 480x640 2 persons, 1 kite, 133.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 3 persons, 1 kite, 133.0ms\n",
      "0: 480x640 3 persons, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "0: 480x640 2 persons, 2 kites, 139.0ms\n",
      "0: 480x640 3 persons, 2 kites, 135.0ms\n",
      "0: 480x640 3 persons, 2 kites, 153.0ms\n",
      "0: 480x640 3 persons, 2 kites, 135.0ms\n",
      "0: 480x640 2 persons, 2 kites, 136.0ms\n",
      "0: 480x640 2 persons, 2 kites, 137.0ms\n",
      "0: 480x640 2 persons, 1 kite, 135.0ms\n",
      "0: 480x640 2 persons, 2 kites, 136.0ms\n",
      "0: 480x640 1 person, 2 kites, 140.0ms\n",
      "0: 480x640 3 persons, 2 kites, 135.0ms\n",
      "0: 480x640 3 persons, 2 kites, 134.0ms\n",
      "0: 480x640 1 person, 2 kites, 134.0ms\n",
      "0: 480x640 1 person, 2 kites, 133.0ms\n",
      "0: 480x640 1 person, 2 kites, 134.0ms\n",
      "0: 480x640 1 person, 2 kites, 134.0ms\n",
      "0: 480x640 1 person, 2 kites, 133.0ms\n",
      "0: 480x640 1 person, 2 kites, 136.0ms\n",
      "0: 480x640 1 person, 2 kites, 134.0ms\n",
      "0: 480x640 1 person, 2 kites, 133.0ms\n",
      "0: 480x640 1 person, 2 kites, 133.0ms\n",
      "0: 480x640 1 person, 1 kite, 151.9ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 133.0ms\n",
      "0: 480x640 1 person, 135.9ms\n",
      "0: 480x640 1 person, 1 kite, 142.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 1 kite, 135.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 2 kites, 134.0ms\n",
      "0: 480x640 1 person, 2 kites, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 1 kite, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 134.0ms\n",
      "0: 480x640 1 person, 135.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 1 umbrella, 134.0ms\n",
      "0: 480x640 (no detections), 133.0ms\n",
      "0: 480x640 1 person, 136.0ms\n",
      "0: 480x640 1 person, 143.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 153.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 (no detections), 144.5ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 140.0ms\n",
      "0: 480x640 (no detections), 144.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 140.0ms\n",
      "0: 480x640 (no detections), 147.0ms\n",
      "0: 480x640 (no detections), 160.0ms\n",
      "0: 480x640 (no detections), 141.0ms\n",
      "0: 480x640 (no detections), 140.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 144.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 140.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 151.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 147.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 142.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 140.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 149.8ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 160.0ms\n",
      "0: 480x640 (no detections), 154.0ms\n",
      "0: 480x640 (no detections), 139.1ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 144.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 140.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 144.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 143.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 156.0ms\n",
      "0: 480x640 (no detections), 140.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 141.0ms\n",
      "0: 480x640 (no detections), 138.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 139.0ms\n",
      "0: 480x640 (no detections), 141.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 140.0ms\n",
      "0: 480x640 (no detections), 151.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n",
      "0: 480x640 (no detections), 136.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 (no detections), 135.0ms\n",
      "0: 480x640 1 umbrella, 134.0ms\n",
      "0: 480x640 (no detections), 137.0ms\n",
      "0: 480x640 (no detections), 134.0ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, conf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\engine\\model.py:174\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    153\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    154\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    An alias for the predict method, enabling the model instance to be callable.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m        (List[ultralytics.engine.results.Results]): A list of prediction results, encapsulated in the Results class.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\engine\\model.py:442\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\_contextlib.py:56\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 56\u001b[0m                 response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:254\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 254\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:142\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    137\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    138\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    141\u001b[0m )\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:456\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 456\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39membed)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:102\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:120\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_once(x, profile, visualize, embed)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:141\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 141\u001b[0m x \u001b[38;5;241m=\u001b[39m m(x)  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    142\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:238\u001b[0m, in \u001b[0;36mC2f.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    239\u001b[0m     y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model(source=0, show=True, conf=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d8d7645-3eef-4411-9067-7a24097efc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.release()\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
