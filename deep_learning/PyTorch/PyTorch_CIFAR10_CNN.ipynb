{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62efe47d-7e49-4929-ac1a-0d0ce1ce6751",
   "metadata": {},
   "source": [
    "<h1>CIFAR-10 Klassifizierung mit CNN</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456db80a-444d-4194-b044-4f7cd8b8e76f",
   "metadata": {},
   "source": [
    "Wie bei TensorFlow ist die Verwendung von PyTorch relative einfach und überschaubar.\n",
    "\n",
    "Das Notebook deckt dasselbe Thema ab, wie das Notebook das mit TensorFlow erstellt wurde.\n",
    "\n",
    "Das CIFAR-10 Dataset hat 10 Klassen und insgesamt 60K Bilder in der Auflösung 32x32. 50K für das Training und 10K für das Testen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec36730-8cd1-406e-aadc-8e112ea4da7c",
   "metadata": {},
   "source": [
    "> The CIFAR-10 dataset\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html [Letzter Zugriff: 09.07.2024]\n",
    ">\n",
    "> Reference:\n",
    "Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed877b98-02db-4a19-bca1-9ddb20d64532",
   "metadata": {},
   "source": [
    "Wie auch bei TensorFlow bietet PyTorch auch Build-In Datasets an, die z. B. für Versuche genutzt werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "425480c1-0688-4360-a7ac-913ca67c479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports.\n",
    "import torch   as pt\n",
    "import torchvision \n",
    "import pandas  as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21850424-6399-498d-bd18-b530702cecd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.18.1+cpu'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad3b8830-083e-4a5c-adbf-09c0e733dd1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1+cpu'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c8a4f5-aacc-4675-8da6-79f121e978aa",
   "metadata": {},
   "source": [
    "<h2>Dataset und Details</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23af7466-1375-41da-9878-46487ffd7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gebe Transformation an.\n",
    "# - Data Argumentation, um Overfitting zu vermeiden. Daten können auch direkt normalisiert werden.\n",
    "# - Siehe:\n",
    "# https://pytorch.org/vision/main/generated/torchvision.transforms.Compose.html\n",
    "# https://pytorch.org/vision/stable/transforms.html\n",
    "# https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html\n",
    "# https://www.geeksforgeeks.org/how-to-normalize-images-in-pytorch/\n",
    "transform = torchvision.transforms.Compose([  # Liste der Transformationen. \n",
    "   # torchvision.transforms. < Transformation >\n",
    "    torchvision.transforms.ToTensor(),  # Bild zu Tensor [c, w, h]\n",
    "    torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.5, 0.5, 0.5)),\n",
    "    torchvision.transforms.RandomRotation(0.2)\n",
    "    # Weitere Angaben.\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78d0df97-138c-4687-a0f6-c7a823f5b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellt direkt Trainingset.\n",
    "# - Siehe Parameter. \n",
    "# - Beim Ersten mal:  download=True\n",
    "trainset = torchvision.datasets.CIFAR10(root=\"./data/datasets\", train=True, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa74e8e1-fe28-4de9-91aa-ce05948f3cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c4010b4f80>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 200x200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.matshow(trainset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25f17c6e-e98c-46fc-b6c8-cb29104b73c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wende Transformation an. \n",
    "trainset = torchvision.datasets.CIFAR10(root=\"./data/datasets\", train=True, download=False,  transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "231966cb-e4bb-49fc-bf96-b60137098d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ./data/datasets\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.485, 0.456, 0.406), std=(0.5, 0.5, 0.5))\n",
       "               RandomRotation(degrees=[-0.2, 0.2], interpolation=nearest, expand=False, fill=0)\n",
       "           )"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf75a12-1b92-4841-96e4-ea646bae93ba",
   "metadata": {},
   "source": [
    "Einer der größten Unterschiede zu TensorFlow ist, dass es standardmäßig eine Trainingsschleife gibt, die beliebig angepasst werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f13cf74c-9b3e-48ac-a475-411ac1fc34f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (tensor([[[-0.5073, -0.6327, -0.5778,  ...,  0.2692,  0.2222,  0.1908],\n",
      "         [-0.8445, -0.9700, -0.8288,  ..., -0.0053, -0.0367, -0.0131],\n",
      "         [-0.7739, -0.8445, -0.5857,  ..., -0.0445, -0.0288, -0.1151],\n",
      "         ...,\n",
      "         [ 0.6614,  0.6065,  0.5829,  ...,  0.2849, -0.5308, -0.5543],\n",
      "         [ 0.4418,  0.3869,  0.4888,  ...,  0.4731, -0.2092, -0.3190],\n",
      "         [ 0.4182,  0.3476,  0.4339,  ...,  0.7241,  0.2143, -0.0053]],\n",
      "\n",
      "        [[-0.4257, -0.5512, -0.5355,  ...,  0.1233,  0.0684,  0.0605],\n",
      "         [-0.7551, -0.9120, -0.8493,  ..., -0.2218, -0.2610, -0.2296],\n",
      "         [-0.7238, -0.8571, -0.7002,  ..., -0.2532, -0.2532, -0.3395],\n",
      "         ...,\n",
      "         [ 0.4213,  0.2880,  0.3507,  ...,  0.1311, -0.6689, -0.6453],\n",
      "         [ 0.1782,  0.0527,  0.2174,  ...,  0.2488, -0.4257, -0.4963],\n",
      "         [ 0.2174,  0.0998,  0.2017,  ...,  0.5311,  0.0135, -0.1904]],\n",
      "\n",
      "        [[-0.3179, -0.4591, -0.4747,  ...,  0.0351, -0.0120, -0.0042],\n",
      "         [-0.6551, -0.8120, -0.8120,  ..., -0.3806, -0.4198, -0.3649],\n",
      "         [-0.6473, -0.8120, -0.7493,  ..., -0.4198, -0.4198, -0.4826],\n",
      "         ...,\n",
      "         [-0.0591, -0.5453, -0.6081,  ..., -0.2630, -0.7571, -0.6551],\n",
      "         [-0.0591, -0.4826, -0.5767,  ..., -0.0747, -0.5453, -0.5453],\n",
      "         [ 0.0978, -0.0747, -0.1296,  ...,  0.2860, -0.1532, -0.2473]]]), 6)\n",
      "1 (tensor([[[ 0.2378,  0.0182, -0.1465,  ..., -0.2563, -0.2876, -0.3504],\n",
      "         [ 0.1280,  0.1673,  0.0104,  ..., -0.2171, -0.3661, -0.4131],\n",
      "         [ 0.1280,  0.1202, -0.0680,  ..., -0.3504, -0.4367, -0.4445],\n",
      "         ...,\n",
      "         [ 0.4025,  0.2535,  0.2378,  ..., -0.6406, -0.4916, -0.2406],\n",
      "         [ 0.3241,  0.2535,  0.2771,  ..., -0.1622, -0.0053,  0.0575],\n",
      "         [ 0.3084,  0.2692,  0.3084,  ...,  0.1516,  0.1516,  0.1516]],\n",
      "\n",
      "        [[ 0.4762,  0.1625, -0.0963,  ..., -0.1669, -0.2061, -0.2767],\n",
      "         [ 0.3429,  0.2880,  0.0684,  ..., -0.1355, -0.2845, -0.3395],\n",
      "         [ 0.3037,  0.2331, -0.0100,  ..., -0.2689, -0.3630, -0.3708],\n",
      "         ...,\n",
      "         [ 0.3978,  0.2958,  0.3429,  ..., -0.6453, -0.4963, -0.2610],\n",
      "         [ 0.2958,  0.2802,  0.3507,  ..., -0.1826, -0.0179,  0.0370],\n",
      "         [ 0.2488,  0.2488,  0.3115,  ...,  0.1311,  0.1390,  0.1311]],\n",
      "\n",
      "        [[ 0.6547,  0.2547, -0.0669,  ..., -0.2551, -0.2551, -0.2630],\n",
      "         [ 0.5135,  0.3958,  0.1135,  ..., -0.2002, -0.3257, -0.3336],\n",
      "         [ 0.4743,  0.3566,  0.0664,  ..., -0.3100, -0.3806, -0.3806],\n",
      "         ...,\n",
      "         [ 0.4900,  0.4429,  0.5213,  ..., -0.5296, -0.3649, -0.0983],\n",
      "         [ 0.1919,  0.2076,  0.3017,  ..., -0.0591,  0.1292,  0.2155],\n",
      "         [ 0.1292,  0.1449,  0.2311,  ...,  0.2782,  0.3017,  0.3174]]]), 9)\n",
      "2 (tensor([[[ 1.0300,  1.0143,  1.0143,  ...,  1.0143,  1.0143,  1.0143],\n",
      "         [ 1.0300,  1.0300,  1.0300,  ...,  1.0300,  1.0300,  1.0300],\n",
      "         [ 1.0300,  1.0222,  1.0222,  ...,  1.0222,  1.0222,  1.0222],\n",
      "         ...,\n",
      "         [-0.0837, -0.0994, -0.1465,  ..., -0.4053, -0.4053, -0.4053],\n",
      "         [-0.0994, -0.1543, -0.1935,  ..., -0.4367, -0.4210, -0.3582],\n",
      "         [-0.1386, -0.1935, -0.2249,  ..., -0.3582, -0.3504, -0.3425]],\n",
      "\n",
      "        [[ 1.0880,  1.0723,  1.0723,  ...,  1.0723,  1.0723,  1.0723],\n",
      "         [ 1.0880,  1.0880,  1.0880,  ...,  1.0880,  1.0880,  1.0880],\n",
      "         [ 1.0880,  1.0802,  1.0802,  ...,  1.0802,  1.0802,  1.0802],\n",
      "         ...,\n",
      "         [ 0.0292,  0.0135, -0.0336,  ..., -0.2767, -0.2845, -0.2845],\n",
      "         [ 0.0135, -0.0414, -0.0806,  ..., -0.3238, -0.3159, -0.2532],\n",
      "         [-0.0257, -0.0806, -0.1120,  ..., -0.2453, -0.2453, -0.2375]],\n",
      "\n",
      "        [[ 1.1880,  1.1723,  1.1723,  ...,  1.1723,  1.1723,  1.1723],\n",
      "         [ 1.1880,  1.1880,  1.1880,  ...,  1.1880,  1.1880,  1.1880],\n",
      "         [ 1.1880,  1.1802,  1.1802,  ...,  1.1802,  1.1802,  1.1802],\n",
      "         ...,\n",
      "         [ 0.0664,  0.0586,  0.0194,  ..., -0.1845, -0.1924, -0.1924],\n",
      "         [ 0.0507,  0.0037, -0.0434,  ..., -0.2395, -0.2238, -0.1689],\n",
      "         [ 0.0115, -0.0434, -0.0747,  ..., -0.1610, -0.1610, -0.1532]]]), 9)\n",
      "3 (tensor([[[-0.7504, -0.6798, -0.6720,  ..., -0.3739, -0.3347, -0.3033],\n",
      "         [-0.7112, -0.7033, -0.7190,  ..., -0.2249, -0.2171, -0.3033],\n",
      "         [-0.6641, -0.6563, -0.5229,  ..., -0.2406, -0.1308, -0.2249],\n",
      "         ...,\n",
      "         [-0.3190, -0.2876, -0.3112,  ..., -0.1935, -0.2641, -0.3347],\n",
      "         [-0.2798, -0.2641, -0.2406,  ..., -0.3425, -0.3739, -0.3269],\n",
      "         [-0.2092, -0.2327, -0.2406,  ..., -0.5465, -0.4759, -0.4053]],\n",
      "\n",
      "        [[-0.7159, -0.6453, -0.6375,  ..., -0.3865, -0.3473, -0.3159],\n",
      "         [-0.6924, -0.6767, -0.7002,  ..., -0.2689, -0.2689, -0.3473],\n",
      "         [-0.6610, -0.6532, -0.5198,  ..., -0.3159, -0.2140, -0.3081],\n",
      "         ...,\n",
      "         [-0.3395, -0.3081, -0.3316,  ..., -0.1826, -0.2532, -0.3238],\n",
      "         [-0.3473, -0.3316, -0.3081,  ..., -0.3316, -0.3630, -0.3159],\n",
      "         [-0.3002, -0.3238, -0.3238,  ..., -0.5434, -0.4728, -0.4022]],\n",
      "\n",
      "        [[-0.7336, -0.6630, -0.6551,  ..., -0.5061, -0.4747, -0.4434],\n",
      "         [-0.7100, -0.7022, -0.7179,  ..., -0.3806, -0.3728, -0.4591],\n",
      "         [-0.6944, -0.6787, -0.5532,  ..., -0.4042, -0.2944, -0.3885],\n",
      "         ...,\n",
      "         [-0.4042, -0.3728, -0.4042,  ..., -0.2630, -0.3336, -0.4042],\n",
      "         [-0.4120, -0.4042, -0.3728,  ..., -0.3963, -0.4277, -0.3806],\n",
      "         [-0.3728, -0.3963, -0.3963,  ..., -0.5924, -0.5218, -0.4512]]]), 4)\n",
      "4 (tensor([[[ 0.3633,  0.3476,  0.4182,  ...,  0.3006,  0.2692,  0.2614],\n",
      "         [ 0.3476,  0.3790,  0.3712,  ...,  0.2771,  0.2535,  0.2378],\n",
      "         [ 0.2378,  0.1986,  0.0418,  ...,  0.2927,  0.2614,  0.2378],\n",
      "         ...,\n",
      "         [-0.3896, -0.3739, -0.3582,  ..., -0.4131, -0.4367, -0.4916],\n",
      "         [-0.4367, -0.4288, -0.4053,  ..., -0.3739, -0.4131, -0.4131],\n",
      "         [-0.4445, -0.4367, -0.4288,  ..., -0.3818, -0.4131, -0.3975]],\n",
      "\n",
      "        [[ 0.4998,  0.4841,  0.5390,  ...,  0.4919,  0.4841,  0.4762],\n",
      "         [ 0.5076,  0.5390,  0.5233,  ...,  0.4762,  0.4684,  0.4527],\n",
      "         [ 0.4213,  0.3821,  0.2174,  ...,  0.4841,  0.4762,  0.4527],\n",
      "         ...,\n",
      "         [-0.2532, -0.2453, -0.2453,  ..., -0.3238, -0.3473, -0.4022],\n",
      "         [-0.3159, -0.3081, -0.2924,  ..., -0.2845, -0.3238, -0.3238],\n",
      "         [-0.3238, -0.3159, -0.3238,  ..., -0.2924, -0.3238, -0.3081]],\n",
      "\n",
      "        [[ 0.7409,  0.7253,  0.7802,  ...,  0.8743,  0.8664,  0.8507],\n",
      "         [ 0.7409,  0.7645,  0.7566,  ...,  0.8507,  0.8429,  0.8272],\n",
      "         [ 0.6468,  0.6076,  0.4586,  ...,  0.8664,  0.8507,  0.8272],\n",
      "         ...,\n",
      "         [-0.1845, -0.1767, -0.1689,  ..., -0.2002, -0.2238, -0.2787],\n",
      "         [-0.2081, -0.2002, -0.2002,  ..., -0.1610, -0.2002, -0.2002],\n",
      "         [-0.2002, -0.1924, -0.2159,  ..., -0.1689, -0.2002, -0.1845]]]), 1)\n"
     ]
    }
   ],
   "source": [
    "stop = 5\n",
    "i=0\n",
    "for x, y in enumerate(trainset):\n",
    "    print(x, y)  # Ausgabe Bild und Klasse. \n",
    "    i+=1\n",
    "    if i ==stop:\n",
    "        break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec51c291-1cb2-46d4-b7f9-016483d1d8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.5073, -0.6327, -0.5778,  ...,  0.2692,  0.2222,  0.1908],\n",
       "          [-0.8445, -0.9700, -0.8288,  ..., -0.0053, -0.0367, -0.0131],\n",
       "          [-0.7739, -0.8445, -0.5857,  ..., -0.0445, -0.0288, -0.1151],\n",
       "          ...,\n",
       "          [ 0.6614,  0.6065,  0.5829,  ...,  0.2849, -0.5308, -0.5543],\n",
       "          [ 0.4418,  0.3869,  0.4888,  ...,  0.4731, -0.2092, -0.3190],\n",
       "          [ 0.4182,  0.3476,  0.4339,  ...,  0.7241,  0.2143, -0.0053]],\n",
       " \n",
       "         [[-0.4257, -0.5512, -0.5355,  ...,  0.1233,  0.0684,  0.0605],\n",
       "          [-0.7551, -0.9120, -0.8493,  ..., -0.2218, -0.2610, -0.2296],\n",
       "          [-0.7238, -0.8571, -0.7002,  ..., -0.2532, -0.2532, -0.3395],\n",
       "          ...,\n",
       "          [ 0.4213,  0.2880,  0.3507,  ...,  0.1311, -0.6689, -0.6453],\n",
       "          [ 0.1782,  0.0527,  0.2174,  ...,  0.2488, -0.4257, -0.4963],\n",
       "          [ 0.2174,  0.0998,  0.2017,  ...,  0.5311,  0.0135, -0.1904]],\n",
       " \n",
       "         [[-0.3179, -0.4591, -0.4747,  ...,  0.0351, -0.0120, -0.0042],\n",
       "          [-0.6551, -0.8120, -0.8120,  ..., -0.3806, -0.4198, -0.3649],\n",
       "          [-0.6473, -0.8120, -0.7493,  ..., -0.4198, -0.4198, -0.4826],\n",
       "          ...,\n",
       "          [-0.0591, -0.5453, -0.6081,  ..., -0.2630, -0.7571, -0.6551],\n",
       "          [-0.0591, -0.4826, -0.5767,  ..., -0.0747, -0.5453, -0.5453],\n",
       "          [ 0.0978, -0.0747, -0.1296,  ...,  0.2860, -0.1532, -0.2473]]]),\n",
       " 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46e5340a-23aa-4f85-b2a3-21964d95efe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18837925-5e54-42dd-a416-e2dd5da9bf86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'plane',\n",
       " 1: 'car',\n",
       " 2: 'bird',\n",
       " 3: 'cat',\n",
       " 4: 'deer',\n",
       " 5: 'dog',\n",
       " 6: 'frog',\n",
       " 7: 'horse',\n",
       " 8: 'ship',\n",
       " 9: 'truck'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Erstelle Map für Klassen.\n",
    "classes = ['plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "class_dict = {}\n",
    "for i in range(len(classes)):\n",
    "    class_dict[i] = classes[i]\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6294c499-4d2b-4d01-adfe-f469870f5e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testset:\n",
    "# - train=False\n",
    "testset     = torchvision.datasets.CIFAR10(root=\"./data/datasets\", train=True, download=False,  transform=transform)\n",
    "\n",
    "# Dataloader haben verschiedene wichtige Parameter. \n",
    "# - Hier nutzen wir nur einige.\n",
    "\n",
    "trainloader = pt.utils.data.DataLoader( trainset )\n",
    "testloader  = pt.utils.data.DataLoader( trainset )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c088a01-b761-4f99-8f26-d0a3c4e97799",
   "metadata": {},
   "source": [
    "Danach erstellen wir das Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18325f-3e04-4af5-832c-30fe4d602e05",
   "metadata": {},
   "source": [
    "<h2>Model und Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38362f4c-36fa-4ae0-ada7-2f60e6e618db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# - Siehe:\n",
    "#   nn:     https://pytorch.org/docs/stable/nn.html\n",
    "#   Conv2D: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\n",
    "#   torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "# in_channels=3, wenn RGB, wenn grau: 1.  \n",
    "# out_channels: n-feature Maps, diese dann als Input nutzen. \n",
    "# kernel_size: größe des Filters. Int oder Tupel. \n",
    "# Zusammengefasst: pt.nn.Conv2d(in_channels,  out_channels, kernel_size, ...) \n",
    "\n",
    "class my_model(pt.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Erstelle verschiedene Schichten.\n",
    "        # - Faltungsnetz \n",
    "        self.conv1 = pt.nn.Conv2d(in_channels=3, out_channels=5, kernel_size=(3, 3))\n",
    "        self.pool  = pt.nn.MaxPool2d(2, 2)  # Pooling. pt.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, ...) \n",
    "        self.conv2 = pt.nn.Conv2d(5, 14, 5)\n",
    "        # - ANN\n",
    "        self.fc1 = pt.nn.Linear(14 * 5 * 5, 100) #  torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "        self.fc2 = pt.nn.Linear(100, 50)\n",
    "        self.fc3 = pt.nn.Linear(50, 10)\n",
    "        \n",
    "    # Forward-Pass.\n",
    "    def forward(self, x):\n",
    "        # Nutze definiertes Pooling und Aktivierungsfunktion. \n",
    "        x = self.pool( pt.nn.functional.relu( self.conv1(x) ))\n",
    "        x = self.pool( pt.nn.functional.relu( self.conv2(x) ))\n",
    "        # Flattern und an ANN anschließen. \n",
    "        x = pt.flatten(x, 1) \n",
    "        x = pt.nn.functional.relu( self.fc1(x))\n",
    "        x = pt.nn.functional.relu( self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67a08cf1-e140-491b-acc0-c4e7a5ab34e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38aa6e88-3b46-48e8-87c9-171a8395347e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_model(\n",
       "  (conv1): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(5, 14, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=350, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (fc3): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a147366-3516-4072-8906-7138952cb109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle Kostenfunktion und Optimierer für Gradientenabstieg.\n",
    "\n",
    "criterion = pt.nn.CrossEntropyLoss()\n",
    "optimizer = pt.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9b7f7ab-85c5-4f4a-a5ca-99ed34180c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    momentum: 0.9\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c45ae7-18c6-45ea-8e77-375a3bc24132",
   "metadata": {},
   "source": [
    "Dann erstellen wir eine Trainingsschleife, wo einiges an Zusätzen dazugepackt werden kann. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87ae6eb1-be6f-4d76-bd0f-32bfe55dad89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.293\n",
      "[1,  4000] loss: 4.392\n",
      "[1,  6000] loss: 6.357\n",
      "[1,  8000] loss: 8.220\n",
      "[1, 10000] loss: 10.028\n",
      "[1, 12000] loss: 11.783\n",
      "[1, 14000] loss: 13.495\n",
      "[1, 16000] loss: 15.206\n",
      "[1, 18000] loss: 16.867\n",
      "[1, 20000] loss: 18.497\n",
      "[1, 22000] loss: 20.128\n",
      "[1, 24000] loss: 21.703\n",
      "[1, 26000] loss: 23.261\n",
      "[1, 28000] loss: 24.812\n",
      "[1, 30000] loss: 26.365\n",
      "[1, 32000] loss: 27.904\n",
      "[1, 34000] loss: 29.525\n",
      "[1, 36000] loss: 31.099\n",
      "[1, 38000] loss: 32.658\n",
      "[1, 40000] loss: 34.204\n",
      "[1, 42000] loss: 35.738\n",
      "[1, 44000] loss: 37.317\n",
      "[1, 46000] loss: 38.866\n",
      "[1, 48000] loss: 40.383\n",
      "[1, 50000] loss: 41.890\n"
     ]
    }
   ],
   "source": [
    "epoches = 2\n",
    "for epoch in range(epoches):   # n-Epochen\n",
    "\n",
    "    epochs_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):  # Iteriere durch Dataset. \n",
    "        \n",
    "        inputs, labels = data\n",
    "\n",
    "        # Setze Gradienten auf 0 für jedes mini-Batch.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)   # Fowards-Pass: Schiebe Daten in das Netz und hole Ergebnis. Siehe: https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "        loss = criterion(outputs, labels)  # Berechne den Fehler mit der angegebenen Kostenfunktion. \n",
    "        loss.backward()                    # Passe Weights durch den angegebenen Gradientenasbtieg an. \n",
    "        \n",
    "        optimizer.step()  # Führe die Anpassung jetzt durch. \n",
    "\n",
    "        # PyTorch liefert von sich aus keine Ausgaben wie TensorFlow.\n",
    "        # - Hier können wir kreative Outputs generieren (oder an jeder anderen Stelle in der Schleife)\n",
    "        epochs_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # Ausgabe alle 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {epochs_loss / 2000:.3f}')\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3711aa-5ee1-4df2-87f1-fa4ffd668a10",
   "metadata": {},
   "source": [
    "Am Ende ohne die Kommentare und Erklärungen sieht es kompakt und sauber aus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f846-7538-4662-b4c6-b239ec3f23b0",
   "metadata": {},
   "source": [
    "<h2>Testen</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb1f6b-6c4a-4095-9083-0695f6496f1f",
   "metadata": {},
   "source": [
    "Als Output bekommen wir einen Tensor. Der Index des höchsten Wertes gibt die Klasse an, die vorhergesagt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "569a9e42-fb62-492b-a161-a30350a58fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.3000]),\n",
       "indices=tensor([2]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index des höchsten Wertes. \n",
    "pt.max(pt.tensor([[ 0.1,  0.2, 0.3]]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1bdf9b1-c9ac-4161-99be-9b604f39da1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.4000]),\n",
       "indices=tensor([1]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.max(pt.tensor([[ 0.2,  0.4, 0.1]]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de6dd37f-553d-4cd0-92a9-8d973bb87b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2708, -2.0719,  1.2771,  2.1320,  0.5370,  1.4362,  1.5946,  0.8822,\n",
      "         -1.6674, -0.8768]])\n"
     ]
    }
   ],
   "source": [
    "with pt.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        print(outputs)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dff6c3-d9e7-4210-a47a-4bae3ad53dde",
   "metadata": {},
   "source": [
    "Als Ausgabe haben wir einen Vektor mit 10 Einträgen, da wir 10 Klassen haben.\n",
    "\n",
    "Damit können wir eine Funktion erstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "72caf6db-c759-4ce4-a9af-e62e6e84ce07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 47 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total   = 0\n",
    "# Damit wird verhindert, dass Gradienten berechnet werden. \n",
    "with pt.no_grad():\n",
    "    for data in testloader:\n",
    "        \n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = pt.max(outputs.data, 1)  # torch.max(input): Gibt höchsten Wert eines Tensors zurück. \n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb7f13-370b-436e-ae7b-d5d66402046d",
   "metadata": {},
   "source": [
    "<h2>Zweiter Aufbau</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0134ad75-cea0-4ec9-acb3-84c456b43826",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model2(pt.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = pt.nn.Conv2d(in_channels=3, out_channels=12, kernel_size=(3, 3))\n",
    "        self.pool  = pt.nn.MaxPool2d(2, 2)  # Pooling. pt.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, ...) \n",
    "        self.conv2 = pt.nn.Conv2d(12, 20, 5)\n",
    "        \n",
    "        self.fc1 = pt.nn.Linear(20 * 5 * 5, 125) #  torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "        self.fc2 = pt.nn.Linear(125, 75)\n",
    "        self.fc3 = pt.nn.Linear(75, 30)\n",
    "        self.fc4 = pt.nn.Linear(30, 10)\n",
    "        \n",
    "    # Forward-Pass.\n",
    "    def forward(self, x):\n",
    "        # Nutze definiertes Pooling und Aktivierungsfunktion. \n",
    "        x = self.pool( pt.nn.functional.relu( self.conv1(x) ))\n",
    "        x = self.pool( pt.nn.functional.relu( self.conv2(x) ))\n",
    "        # Flattern und an ANN anschließen. \n",
    "        x = pt.flatten(x, 1) \n",
    "        x = pt.nn.functional.relu( self.fc1(x))\n",
    "        x = pt.nn.functional.relu( self.fc2(x))\n",
    "        x = pt.nn.functional.relu( self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d3afe5e6-b7a0-4b07-a2b1-26693ce36277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    momentum: 0.9\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = pt.nn.CrossEntropyLoss()\n",
    "optimizer = pt.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "eda4b94d-ba62-4513-938e-55e954a4ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_model2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e18825c3-01c3-4646-9e9d-babe8abe6b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    momentum: 0.9\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = pt.nn.CrossEntropyLoss()\n",
    "optimizer = pt.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fcf0e949-bd94-4f82-9072-e882c92ee93a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.305\n",
      "[1,  4000] loss: 4.566\n",
      "[1,  6000] loss: 6.655\n",
      "[1,  8000] loss: 8.610\n",
      "[1, 10000] loss: 10.510\n",
      "[1, 12000] loss: 12.324\n",
      "[1, 14000] loss: 14.098\n",
      "[1, 16000] loss: 15.847\n",
      "[1, 18000] loss: 17.542\n",
      "[1, 20000] loss: 19.224\n",
      "[1, 22000] loss: 20.863\n",
      "[1, 24000] loss: 22.455\n",
      "[1, 26000] loss: 24.003\n",
      "[1, 28000] loss: 25.564\n",
      "[1, 30000] loss: 27.086\n",
      "[1, 32000] loss: 28.643\n",
      "[1, 34000] loss: 30.226\n",
      "[1, 36000] loss: 31.757\n",
      "[1, 38000] loss: 33.308\n",
      "[1, 40000] loss: 34.825\n",
      "[1, 42000] loss: 36.312\n",
      "[1, 44000] loss: 37.837\n",
      "[1, 46000] loss: 39.319\n",
      "[1, 48000] loss: 40.772\n",
      "[1, 50000] loss: 42.219\n",
      "[2,  2000] loss: 1.433\n",
      "[2,  4000] loss: 2.917\n",
      "[2,  6000] loss: 4.372\n",
      "[2,  8000] loss: 5.813\n",
      "[2, 10000] loss: 7.223\n",
      "[2, 12000] loss: 8.671\n",
      "[2, 14000] loss: 10.103\n",
      "[2, 16000] loss: 11.552\n",
      "[2, 18000] loss: 12.962\n",
      "[2, 20000] loss: 14.405\n",
      "[2, 22000] loss: 15.835\n",
      "[2, 24000] loss: 17.196\n",
      "[2, 26000] loss: 18.552\n",
      "[2, 28000] loss: 19.907\n",
      "[2, 30000] loss: 21.250\n",
      "[2, 32000] loss: 22.595\n",
      "[2, 34000] loss: 24.010\n",
      "[2, 36000] loss: 25.432\n",
      "[2, 38000] loss: 26.816\n",
      "[2, 40000] loss: 28.175\n",
      "[2, 42000] loss: 29.523\n",
      "[2, 44000] loss: 30.912\n",
      "[2, 46000] loss: 32.236\n",
      "[2, 48000] loss: 33.565\n",
      "[2, 50000] loss: 34.910\n"
     ]
    }
   ],
   "source": [
    "epoches = 2\n",
    "for epoch in range(epoches):   # n-Epochen\n",
    "\n",
    "    epochs_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):  # Iteriere durch Dataset. \n",
    "        \n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "     \n",
    "        outputs =  model(inputs)\n",
    "        loss = criterion(outputs, labels)   \n",
    "        loss.backward()                    \n",
    "        \n",
    "        optimizer.step()  # Führe die Anpassung jetzt durch. \n",
    "\n",
    "        epochs_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # Ausgabe alle 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {epochs_loss / 2000:.3f}')\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ef5de2ad-7117-4f5b-8a7e-c4dcbdacb93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 53 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total   = 0\n",
    "\n",
    "with pt.no_grad():\n",
    "    for data in testloader:\n",
    "        \n",
    "        images, labels = data\n",
    "        outputs = pt.softmax(model(images), dim=1)\n",
    "        _, predicted = pt.max(outputs.data, 1)  \n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1130e33e-2083-4038-b989-81ec172b9598",
   "metadata": {},
   "source": [
    "<h2>F1-Score</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c3fe6-27a1-474c-948d-85bc22f4c63e",
   "metadata": {},
   "source": [
    "Oder auch so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "dcd9efc4-dcbe-4ccc-ae1d-f0a8aa80f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with pt.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = pt.softmax(model(images), dim=1)\n",
    "        y_true.append(labels.numpy())\n",
    "        y_pred.append(outputs.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "35711658-0960-4cb2-94da-f16f9fce7c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00303583, 0.0115311 , 0.0538298 , 0.33522657, 0.08058397,\n",
       "        0.14503089, 0.29600284, 0.04959843, 0.00312306, 0.02203758]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f12899eb-0247-4222-9881-948e0482b15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "89fb3d9c-048a-4729-a681-e4eb228ae979",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_argmax = [np.argmax(x)   for x in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a80f312b-bb98-44f6-8b4b-8006f03f2539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.42      0.51      5000\n",
      "           1       0.60      0.78      0.68      5000\n",
      "           2       0.46      0.36      0.40      5000\n",
      "           3       0.33      0.39      0.36      5000\n",
      "           4       0.47      0.35      0.40      5000\n",
      "           5       0.49      0.37      0.42      5000\n",
      "           6       0.70      0.59      0.64      5000\n",
      "           7       0.48      0.79      0.60      5000\n",
      "           8       0.57      0.76      0.65      5000\n",
      "           9       0.73      0.54      0.62      5000\n",
      "\n",
      "    accuracy                           0.54     50000\n",
      "   macro avg       0.55      0.54      0.53     50000\n",
      "weighted avg       0.55      0.54      0.53     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, pred_argmax))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
