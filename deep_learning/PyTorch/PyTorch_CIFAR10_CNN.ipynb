{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62efe47d-7e49-4929-ac1a-0d0ce1ce6751",
   "metadata": {},
   "source": [
    "<h1>CIFAR-10 Klassifizierung mit CNN</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456db80a-444d-4194-b044-4f7cd8b8e76f",
   "metadata": {},
   "source": [
    "Wie bei TensorFlow ist die Verwendung von PyTorch relative einfach und überschaubar.\n",
    "\n",
    "Das Notebook deckt dasselbe Thema ab, wie das Notebook das mit TensorFlow erstellt wurde.\n",
    "\n",
    "Das CIFAR-10 Dataset hat 10 Klassen und insgesamt 60K Bilder in der Auflösung 32x32. 50K für das Training und 10K für das Testen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec36730-8cd1-406e-aadc-8e112ea4da7c",
   "metadata": {},
   "source": [
    "> The CIFAR-10 dataset\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html [Letzter Zugriff: 09.07.2024]\n",
    ">\n",
    "> Reference:\n",
    "Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed877b98-02db-4a19-bca1-9ddb20d64532",
   "metadata": {},
   "source": [
    "Wie auch bei TensorFlow bietet PyTorch auch Build-In Datasets an, die z. B. für Versuche genutzt werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "425480c1-0688-4360-a7ac-913ca67c479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports.\n",
    "import torch   as pt\n",
    "import torchvision \n",
    "import pandas  as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21850424-6399-498d-bd18-b530702cecd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.18.1+cpu'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ad3b8830-083e-4a5c-adbf-09c0e733dd1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1+cpu'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c8a4f5-aacc-4675-8da6-79f121e978aa",
   "metadata": {},
   "source": [
    "<h2>Dataset und Details</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "23af7466-1375-41da-9878-46487ffd7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gebe Transformation an.\n",
    "# - Data Argumentation, um Overfitting zu vermeiden. Daten können auch direkt normalisiert werden.\n",
    "# - Siehe:\n",
    "# https://pytorch.org/vision/main/generated/torchvision.transforms.Compose.html\n",
    "# https://pytorch.org/vision/stable/transforms.html\n",
    "# https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html\n",
    "# https://www.geeksforgeeks.org/how-to-normalize-images-in-pytorch/\n",
    "transform = torchvision.transforms.Compose([  # Liste der Transformationen. \n",
    "   # torchvision.transforms. < Transformation >\n",
    "    torchvision.transforms.ToTensor(),  # Bild zu Tensor [c, w, h]\n",
    "    torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.5, 0.5, 0.5)),\n",
    "    torchvision.transforms.RandomRotation(0.2)\n",
    "    # Weitere Angaben.\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "78d0df97-138c-4687-a0f6-c7a823f5b489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Erstellt direkt Trainingset.\n",
    "# - Siehe Parameter. \n",
    "trainset = torchvision.datasets.CIFAR10(root=\"./data/datasets\", train=True, download=True,  transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "231966cb-e4bb-49fc-bf96-b60137098d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ./data/datasets\n",
       "    Split: Train"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf75a12-1b92-4841-96e4-ea646bae93ba",
   "metadata": {},
   "source": [
    "Einer der größten Unterschiede zu TensorFlow ist, dass es standardmäßig eine Trainingsschleife gibt, die beliebig angepasst werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f13cf74c-9b3e-48ac-a475-411ac1fc34f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (<PIL.Image.Image image mode=RGB size=32x32 at 0x222455E2D50>, 6)\n",
      "1 (<PIL.Image.Image image mode=RGB size=32x32 at 0x222455E0A10>, 9)\n",
      "2 (<PIL.Image.Image image mode=RGB size=32x32 at 0x222455E2D50>, 9)\n",
      "3 (<PIL.Image.Image image mode=RGB size=32x32 at 0x222455E0A10>, 4)\n",
      "4 (<PIL.Image.Image image mode=RGB size=32x32 at 0x222455E2D50>, 1)\n"
     ]
    }
   ],
   "source": [
    "stop = 5\n",
    "i=0\n",
    "for x, y in enumerate(trainset):\n",
    "    print(x, y)  # Ausgabe Bild und Klasse. \n",
    "    i+=1\n",
    "    if i ==stop:\n",
    "        break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec51c291-1cb2-46d4-b7f9-016483d1d8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=32x32>, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a79e4a8-0521-45af-86fc-23011d2abb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22247608e60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(trainset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "18837925-5e54-42dd-a416-e2dd5da9bf86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'plane',\n",
       " 1: 'car',\n",
       " 2: 'bird',\n",
       " 3: 'cat',\n",
       " 4: 'deer',\n",
       " 5: 'dog',\n",
       " 6: 'frog',\n",
       " 7: 'horse',\n",
       " 8: 'ship',\n",
       " 9: 'truck'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "class_dict = {}\n",
    "for i in range(len(classes)):\n",
    "    class_dict[i] = classes[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6294c499-4d2b-4d01-adfe-f469870f5e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Testset:\n",
    "# - train=False\n",
    "testset     = torchvision.datasets.CIFAR10(root=\"./data/datasets\", train=True, download=True,  transform=transform)\n",
    "\n",
    "# Dataloader haben verschiedene wichtige Parameter. \n",
    "# - Hier nutzen wir nur einige.\n",
    "\n",
    "trainloader = pt.utils.data.DataLoader( trainset )\n",
    "testloader  = pt.utils.data.DataLoader( trainset )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c088a01-b761-4f99-8f26-d0a3c4e97799",
   "metadata": {},
   "source": [
    "Danach erstellen wir das Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "58182b0f-06dd-4f02-85cc-eaf94ad39de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = pt.nn.MaxPool2d(2, 2)\n",
    "layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18325f-3e04-4af5-832c-30fe4d602e05",
   "metadata": {},
   "source": [
    "<h2>Model und Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "38362f4c-36fa-4ae0-ada7-2f60e6e618db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# - Siehe:\n",
    "#   nn:     https://pytorch.org/docs/stable/nn.html\n",
    "#   Conv2D: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\n",
    "#   torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "# in_channels=3, wenn RGB, wenn grau: 1.  \n",
    "# out_channels: n-feature Maps, diese dann als Input nutzen. \n",
    "# kernel_size: größe des Filters. Int oder Tupel. \n",
    "# Zusammengefasst: pt.nn.Conv2d(in_channels,  out_channels, kernel_size, ...) \n",
    "\n",
    "class my_model(pt.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Erstelle verschiedene Schichten.\n",
    "        # - Faltungsnetz \n",
    "        self.conv1 = pt.nn.Conv2d(in_channels=3, out_channels=5, kernel_size=(3, 3))\n",
    "        self.pool  = pt.nn.MaxPool2d(2, 2)  # Pooling. pt.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, ...) \n",
    "        self.conv2 = pt.nn.Conv2d(5, 14, 5)\n",
    "        # - ANN\n",
    "        self.fc1 = pt.nn.Linear(14 * 5 * 5, 100) #  torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "        self.fc2 = pt.nn.Linear(100, 50)\n",
    "        self.fc3 = pt.nn.Linear(50, 10)\n",
    "        \n",
    "    # Forward-Pass.\n",
    "    def forward(self, x):\n",
    "        # Nutze definiertes Pooling und Aktivierungsfunktion. \n",
    "        x = self.pool( pt.nn.functional.relu( self.conv1(x) ))\n",
    "        x = self.pool( pt.nn.functional.relu( self.conv2(x) ))\n",
    "        # Flattern und an ANN anschließen. \n",
    "        x = pt.flatten(x, 1) \n",
    "        x = pt.nn.functional.relu( self.fc1(x))\n",
    "        x = pt.nn.functional.relu( self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "67a08cf1-e140-491b-acc0-c4e7a5ab34e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "38aa6e88-3b46-48e8-87c9-171a8395347e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_model(\n",
       "  (conv1): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(5, 14, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=350, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (fc3): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5a147366-3516-4072-8906-7138952cb109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle Kostenfunktion und Optimierer für Gradientenabstieg.\n",
    "\n",
    "criterion = pt.nn.CrossEntropyLoss()\n",
    "optimizer = pt.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c9b7f7ab-85c5-4f4a-a5ca-99ed34180c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    momentum: 0.9\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c45ae7-18c6-45ea-8e77-375a3bc24132",
   "metadata": {},
   "source": [
    "Dann erstellen wir eine Trainingsschleife, wo einiges an Zusätzen dazugepackt werden kann. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "87ae6eb1-be6f-4d76-bd0f-32bfe55dad89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.057\n",
      "[1,  4000] loss: 4.032\n",
      "[1,  6000] loss: 5.896\n",
      "[1,  8000] loss: 7.705\n",
      "[1, 10000] loss: 9.493\n",
      "[1, 12000] loss: 11.217\n",
      "[1, 14000] loss: 12.915\n",
      "[1, 16000] loss: 14.638\n",
      "[1, 18000] loss: 16.317\n",
      "[1, 20000] loss: 17.992\n",
      "[1, 22000] loss: 19.621\n",
      "[1, 24000] loss: 21.208\n",
      "[1, 26000] loss: 22.802\n",
      "[1, 28000] loss: 24.387\n",
      "[1, 30000] loss: 25.959\n",
      "[1, 32000] loss: 27.555\n",
      "[1, 34000] loss: 29.204\n",
      "[1, 36000] loss: 30.783\n",
      "[1, 38000] loss: 32.355\n",
      "[1, 40000] loss: 33.926\n",
      "[1, 42000] loss: 35.491\n",
      "[1, 44000] loss: 37.053\n",
      "[1, 46000] loss: 38.598\n",
      "[1, 48000] loss: 40.141\n",
      "[1, 50000] loss: 41.647\n",
      "[2,  2000] loss: 1.525\n",
      "[2,  4000] loss: 3.066\n",
      "[2,  6000] loss: 4.590\n",
      "[2,  8000] loss: 6.114\n",
      "[2, 10000] loss: 7.622\n",
      "[2, 12000] loss: 9.117\n",
      "[2, 14000] loss: 10.627\n",
      "[2, 16000] loss: 12.150\n",
      "[2, 18000] loss: 13.649\n",
      "[2, 20000] loss: 15.150\n",
      "[2, 22000] loss: 16.642\n",
      "[2, 24000] loss: 18.077\n",
      "[2, 26000] loss: 19.519\n",
      "[2, 28000] loss: 20.975\n",
      "[2, 30000] loss: 22.440\n",
      "[2, 32000] loss: 23.914\n",
      "[2, 34000] loss: 25.460\n",
      "[2, 36000] loss: 26.950\n",
      "[2, 38000] loss: 28.423\n",
      "[2, 40000] loss: 29.890\n",
      "[2, 42000] loss: 31.354\n",
      "[2, 44000] loss: 32.855\n",
      "[2, 46000] loss: 34.318\n",
      "[2, 48000] loss: 35.757\n",
      "[2, 50000] loss: 37.211\n"
     ]
    }
   ],
   "source": [
    "epoches = 1\n",
    "for epoch in range(epoches):   # n-Epochen\n",
    "\n",
    "    epochs_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):  # Iteriere durch Dataset. \n",
    "        \n",
    "        inputs, labels = data\n",
    "\n",
    "        # Setze Gradienten auf 0 für jedes mini-Batch.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)   # Fowards-Pass: Schiebe Daten in das Netz und hole Ergebnis. Siehe: https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "        loss = criterion(outputs, labels)  # Berechne den Fehler mit der angegebenen Kostenfunktion. \n",
    "        loss.backward()                    # Passe Weights durch den angegebenen Gradientenasbtieg an. \n",
    "        \n",
    "        optimizer.step()  # Führe die Anpassung jetzt durch. \n",
    "\n",
    "        # PyTorch liefert von sich aus keine Ausgaben wie TensorFlow.\n",
    "        # - Hier können wir kreative Outputs generieren (oder an jeder anderen Stelle in der Schleife)\n",
    "        epochs_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # Ausgabe alle 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {epochs_loss / 2000:.3f}')\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3711aa-5ee1-4df2-87f1-fa4ffd668a10",
   "metadata": {},
   "source": [
    "Am Ende ohne die Kommentare und Erklärungen sieht es kompakt und sauber aus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f846-7538-4662-b4c6-b239ec3f23b0",
   "metadata": {},
   "source": [
    "<h2>Testen</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb1f6b-6c4a-4095-9083-0695f6496f1f",
   "metadata": {},
   "source": [
    "Als Output bekommen wir einen Tensor. Der Index des höchsten Wertes gibt die Klasse an, die vorhergesagt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "569a9e42-fb62-492b-a161-a30350a58fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.3000]),\n",
       "indices=tensor([2]))"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index des höchsten Wertes. \n",
    "pt.max(pt.tensor([[ 0.1,  0.2, 0.3]]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d1bdf9b1-c9ac-4161-99be-9b604f39da1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.4000]),\n",
       "indices=tensor([1]))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.max(pt.tensor([[ 0.2,  0.4, 0.1]]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "de6dd37f-553d-4cd0-92a9-8d973bb87b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0462, -1.3057,  0.8372,  2.4529,  0.6417,  1.7269,  2.6788,  0.3769,\n",
      "         -2.4192, -1.3475]])\n"
     ]
    }
   ],
   "source": [
    "with pt.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        print(outputs)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dff6c3-d9e7-4210-a47a-4bae3ad53dde",
   "metadata": {},
   "source": [
    "Als Ausgabe haben wir einen Vektor mit 10 Einträgen, da wir 10 Klassen haben.\n",
    "\n",
    "Damit können wir eine Funktion erstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "72caf6db-c759-4ce4-a9af-e62e6e84ce07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 51 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total   = 0\n",
    "# Damit wird verhindert, dass Gradienten berechnet werden. \n",
    "with pt.no_grad():\n",
    "    for data in testloader:\n",
    "        \n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = pt.max(outputs.data, 1)  # torch.max(input): Gibt höchsten Wert eines Tensors zurück. \n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb7f13-370b-436e-ae7b-d5d66402046d",
   "metadata": {},
   "source": [
    "<h2>Zweiter Aufbau</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdaae7c-9ec2-497f-848a-35a8f0e119d7",
   "metadata": {},
   "source": [
    "Jetzt nutzen wir Softmax und ein paar andere Einstellungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d00ab-09f3-4dee-a279-b1249deeaa38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "48d20e00-37c1-413f-ab17-4c75ebb05a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = pt.nn.Softmax(dim=1)\n",
    "input = pt.tensor([[2.0, 2.0]])\n",
    "input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "4a6108b3-abfc-48f6-8319-8e855c058c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0134ad75-cea0-4ec9-acb3-84c456b43826",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model2(pt.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = pt.nn.Conv2d(in_channels=3, out_channels=5, kernel_size=(3, 3))\n",
    "        self.pool  = pt.nn.MaxPool2d(2, 2)  \n",
    "        self.conv2 = pt.nn.Conv2d(5, 14, 5)\n",
    "        self.fc1 = pt.nn.Linear(14 * 5 * 5, 100) \n",
    "        self.fc2 = pt.nn.Linear(100, 50)\n",
    "        self.fc3 = pt.nn.Linear(50, 10)\n",
    "\n",
    "        self.softmax_layer =  pt.nn.Softmax(dim=1)\n",
    "    # Forward-Pass.\n",
    "    def forward(self, x):\n",
    "        x = self.pool( pt.nn.functional.relu( self.conv1(x) ))\n",
    "        x = self.pool( pt.nn.functional.relu( self.conv2(x) ))\n",
    "\n",
    "        x = pt.flatten(x, 1) \n",
    "        x = pt.nn.functional.relu( self.fc1(x))\n",
    "        x = pt.nn.functional.relu( self.fc2(x))\n",
    "        x = self.softmax_layer( self.fc3(x) )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "eda4b94d-ba62-4513-938e-55e954a4ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_model2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e18825c3-01c3-4646-9e9d-babe8abe6b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = pt.nn.CrossEntropyLoss()\n",
    "optimizer = pt.optim.Adam(model.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
