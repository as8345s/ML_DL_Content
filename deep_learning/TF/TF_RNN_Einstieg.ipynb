{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912326dd-33c2-4ebc-8d75-201e08afab70",
   "metadata": {},
   "source": [
    "<h1>RNN - Einstieg</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83394ed-ba97-4c24-badc-a37c7197fd52",
   "metadata": {},
   "source": [
    "RNN steht für Recurrent Neural Network und beinhaltet verschiedene Varianten.\n",
    "\n",
    "Der Hauptunterschied zu normale Netze und CNNs ist das RNNs eine Art Gedächtnis aufweisen. <br>\n",
    "Das Vanilla-RNN leidet deutlich unter dem Vanishing-Gradient Problem, mehr dazu später.\n",
    "\n",
    "RNNs sind sehr gut dafür geeignet mit Daten zu arbeiten die eine bestimmte Sequenz aufweisen, wie das Vorhersagen des nächsten Wortes eines Satzes basierend auf den schon geschriebenen Wörter => Autovervollständigung.\n",
    "\n",
    "Das Gedächtnis entsteht dadurch, das der Output zum Input geführt wird- um es einfach auszudrücken.\n",
    "\n",
    "Use-Cases sind z B.:\n",
    "- NLP\n",
    "- Übersetzungen\n",
    "- Autovervollständigung\n",
    "- NER (Named Entity Recognition)\n",
    "- Semantische Analysen von Texten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80904453-354a-4003-b8df-a863dd6b7223",
   "metadata": {},
   "source": [
    "<i>Abb1</i>: RNN Use-Case Beispiele.\n",
    "\n",
    "<img src=\"./data/img/1_rnn.PNG\" height=800 width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae945969-51f1-4eff-b7ff-129173c7f445",
   "metadata": {},
   "source": [
    "Wieso nutzt man keine normalen Netze für diese Probleme? <br>\n",
    "Alle diese genannten Probleme sind <u>Sequential-Modelling-Problems,</u> weil die <u>Sequenz</u> wichtig ist.\n",
    "\n",
    "Eine Sequenz ist z. B.: \"How are you\", was sich von \"Are you how\" unterscheidet. Für uns Menschen ist das deutlich sichtbar.\n",
    "\n",
    "Weitere Issues mit normale Netzen:\n",
    "- Wie viele Output-Neuronen?\n",
    "- Zu viele Berechnungen, da bei Texten ein Vokabular benutzt wird (Siehe Text in numerischer Darstellung).<br>\n",
    "  Für 30.000 Wörter ist ein Vektor der Größe 30.000 notwendig + Output Größe. <br>\n",
    "  Vokabular:  how: [0, 0, 1, ...], are: [0, 1, 0, ...], ... (On-Hot Encoding)\n",
    "- Keine geteilten Parameter: mehrere Deutungen von Texten.\n",
    "\n",
    "Bei Texten ist die Inputreihenfolge von Wörtern auch entscheidend, nicht wie bei normalen Features wie Eigenschaften, um etwas zu klassifizieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88ea755-69e5-45e1-84e9-0255a7192081",
   "metadata": {},
   "source": [
    "<i>Abb2</i>: Vanilla-RNN Veranschaulichung. Kontext / Memory.\n",
    "\n",
    "<img src=\"./data/img/2_rnn.PNG\" height=800 width=600>\n",
    "\n",
    "\n",
    "<i>Abb3</i>: NER Beispiel.\n",
    "\n",
    "<img src=\"./data/img/3_rnn.PNG\" height=750 width=550>\n",
    "\n",
    "\n",
    "<i>Abb4</i>: ENR Training. \n",
    "\n",
    "<img src=\"./data/img/5_rnn.PNG\" height=700 width=400>\n",
    "\n",
    "\n",
    "<i>Abb5</i>: Generische Darstellung von RNN. \n",
    "\n",
    "<img src=\"./data/img/4_rnn.PNG\" height=160 width=80>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9903ccf-6d0f-4ab8-b8e8-c286a76c18ed",
   "metadata": {},
   "source": [
    "Wie in der Abbildung 2 zusehen ist, wird der Output des Layers wieder zum Input geleitet. Bei Texten kann dann so ein gewisser Kontext mitgenommen werden => die vorherigen Wörter haben einen Einfluss.\n",
    "- Der Zustand wird mitgenommen.\n",
    "\n",
    "Das Training eines RNNs für NER würde so aussehen.:\n",
    "- Nehme Text markiere Namen <br>\n",
    "  \"Bob told me David is home\" => [1, 0, 0, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d236317-3706-4961-9446-22bda67a6172",
   "metadata": {},
   "source": [
    "Wir haben gesehen wie ein einfaches ERN trainiert wird, wie funktioniert das aber bei einer Übersetzung?\n",
    "- Hier müssen alle Wörter vorhanden sein, damit eine Vorhersage gemacht werden kann. Da jedes Wort die Bedeutung und Übersetzung verändert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e358a78-f8e3-498e-aedc-f59f18cbb6b2",
   "metadata": {},
   "source": [
    "<i>Abb5</i>: Übersetzung: Encoder und Decoder.\n",
    "\n",
    "<img src=\"./data/img/6_rnn.PNG\" height=700 width=500>\n",
    "\n",
    "- In diesem Beispiel Single-Layer.\n",
    "\n",
    "\n",
    "<i>Abb7</i>: Deep RNN.\n",
    "\n",
    "<img src=\"./data/img/7_rnn.PNG\" height=600 width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b85090-d477-4971-b913-babb79a41290",
   "metadata": {},
   "source": [
    "<h2>Arten von RNN (3)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1acd58-8ee0-43db-aed9-ea5c98c902e4",
   "metadata": {},
   "source": [
    "RNN Typen.:\n",
    "- Many-to-Many\n",
    "- Many-to-One\n",
    "- One-to-Many"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0a7b9-ea47-4ab6-9419-6bc934854159",
   "metadata": {},
   "source": [
    "Many-to-Many:<br>\n",
    "Viele Inputs und viele Outputs => x(n) und ý(n). Ein Beispiel dafür wäre ENR oder Übersetzungen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf56b08b-5ee4-4150-9744-96a625dfc5c7",
   "metadata": {},
   "source": [
    "<i>Abb8</i>: ENR, Many-to-Many.\n",
    "\n",
    "<img src=\"./data/img/8_rnn.PNG\" height=600 width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf8fa2-ba55-40e0-8356-b2e81420d1c2",
   "metadata": {},
   "source": [
    "Many-to-One:<br>\n",
    "Viele Inputs liefern am Ende ein Output. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d4aa26-f2c6-42a2-879c-e6ca03ecc493",
   "metadata": {},
   "source": [
    "<i>Abb9</i>: Many-to-One.\n",
    "\n",
    "<img src=\"./data/img/9_rnn.PNG\" height=600 width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffb1479-e7e6-452f-bc10-aeeaffc90001",
   "metadata": {},
   "source": [
    "One-to-Many: <br>\n",
    "Für eine Eingabe gibt es viele Outputs. Ein Beispiel dafür wäre Musikgeneration- mit einem gegebenen Seed oder Sample soll eine Melodie generiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c329ab1-b8a2-4247-a590-2017b4ea06c1",
   "metadata": {},
   "source": [
    "<i>Abb10</i>: One-to-Many.\n",
    "\n",
    "<img src=\"./data/img/10_rnn.PNG\" height=600 width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa77c8d-0f03-4ee1-b12e-fb559f6f1642",
   "metadata": {},
   "source": [
    "<h1>Vanising- und Exploding-Gradient</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a00107-4e4f-4094-8a35-fd668d3bc8a9",
   "metadata": {},
   "source": [
    "Das ist einer der Probleme die es bei neuralen Netzen gibt. <br>\n",
    "\n",
    "$\n",
    "weights = weights - Stepsize * gradient\n",
    "$\n",
    "- weights: Model Parameter\n",
    "- Stepsize: Schrittweite des Gradientenabstiegs mit einem gegebenen Verfahren (Siehe SGD, Adam, ...)\n",
    "- Gradient: Berechneter Gradient an der Stelle. <br>\n",
    "  Ableitung Loss- wie sehr sich ein Parameter auf den Fehler am Ende auswirkt <br>\n",
    "  -> Wie verändert sich der Loss bei einer Veränderung der Parameter. \n",
    "\n",
    "Vanishing-Gradient: Veränderungen sind sehr gering => Gewichte ändern sich äußerst gering. <br>\n",
    "Exploding-Gradient: Veränderung sind sehr groß => Gewichte ändern sich mit Updates äußerst stark. \n",
    "\n",
    "Wichtiger Faktor, wenn es um <b>Backpropagation</b> geht.\n",
    "- Siehe: Backpropagation und Kettenregel.\n",
    "\n",
    "Je mehr Layers wir haben, desto mehr Multiplikationen haben wir.\n",
    "- $Gradient = d1 * d2 * ... * dn  $  Bei der Multiplikationen von kleinen Werten $ Gradient = 0.03 * 0.05 = 0.0015$\n",
    "\n",
    "Das zieht sich über das ganze Netzwerk => Parameter der hinteren Layers ändern sich sehr wenig. Der gegenteilige Effekt wäre der Exploding-Gradient\n",
    "\n",
    "Wenn die Anzahl an Layers steigt, wird der Vanising-Gradient immer mehr zum Problem, was das \"Lernen\" negativ beeinflusst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b713b0-4836-4bf2-afba-357a02bbe9e1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Bei RNNs ist Vanising-Gradient Problem eher präsent.\n",
    "\n",
    "\n",
    "Beispiel: Autovervollständigung.<br>\n",
    "Text 1: <u>Heute</u>, wegen meiner Jobsituation und der Familie, <u>muss</u> ich um eine Gehaltserhöhung fragen. <br>\n",
    "Text 2: <u>Letztes Jahr</u>, wegen meiner Jobsituation und der Familie, <u>musste</u> ich um eine Gehaltserhöhung fragen. <br>\n",
    "Dabei ist  <b>\"</b>muss ich um eine Gehaltserhöhung fragen<b>\"</b> der vorhergesagte Text. Das <b>muss</b> und <b>musste</b> bezieht sich auf die Anfangswörter <b>Heute</b> und <b>Letztes Jahr</b>. <br>\n",
    "Wörter die hinten stehen habe ein Einfluss auf Wörter die viel weiter vorne stehen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b023d588-328d-4122-81de-5edcccf88245",
   "metadata": {},
   "source": [
    "<img src=\"./data/img/6_rnn.PNG\" height=700 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2322d876-4d6b-4a42-a49d-0a6e1cc97544",
   "metadata": {},
   "source": [
    "Da die Wörter nacheinander eingegeben werden, wird der Vanising-Gradient mehr zum Problem, da der <u>Effekt</u> der Aktivierung der vergangenen Eingaben z. B. $a^{(2)}$ nachlässt/reduziert  ist, wenn wir schon bei  $a^{(5)}$ sind. Daher kommt der Term <b>Short-Memory.</b> Dadurch ist das Predicten der Wörter <b>muss</b> und <b>musste</b> schwierig.\n",
    "\n",
    "Vanilla-RNNs haben ein kurzes Gedächtnis.\n",
    "- Genau dieses Problem des Vanising-Gradient ist die Ursache für das Short-Memory Problem.\n",
    "- Durch Backpropagation ändern sich die Parameter geringfügig, dadurch kommt dieser Effekt zustande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0283b8-a57f-4086-ab0e-bc5a939ae3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db663e1e-70c7-4103-9c2f-b7018770a327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e76d1-8b48-4ab1-b59f-71bc0743b750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f382063-2fce-4a17-8463-798105f1fcdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba3924-e9d0-43fb-acce-6b399bf11b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daa8cc8-8922-45fa-8559-c13f9ede4912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
