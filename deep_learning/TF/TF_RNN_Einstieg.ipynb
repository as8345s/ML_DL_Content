{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912326dd-33c2-4ebc-8d75-201e08afab70",
   "metadata": {},
   "source": [
    "<h1>RNN - Einstieg</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83394ed-ba97-4c24-badc-a37c7197fd52",
   "metadata": {},
   "source": [
    "RNN steht für Recurrent Neural Network und beinhaltet verschiedene Varianten.\n",
    "\n",
    "Der Hauptunterschied zu normale Netze und CNNs ist das RNNs eine Art Gedächtnis aufweisen. <br>\n",
    "Das Vanilla-RNN leidet deutlich unter dem Vanishing-Gradient Problem, mehr dazu später.\n",
    "\n",
    "RNNs sind sehr gut dafür geeignet mit Daten zu arbeiten die eine bestimmte Sequenz aufweisen, wie das Vorhersagen des nächsten Wortes eines Satzes basierend auf den schon geschriebenen Wörter => Autovervollständigung.\n",
    "\n",
    "Das Gedächtnis entsteht dadurch, das der Output zum Input geführt wird- um es einfach auszudrücken.\n",
    "\n",
    "Use-Cases sind z B.:\n",
    "- NLP\n",
    "- Übersetzungen\n",
    "- Autovervollständigung\n",
    "- NER (Named Entity Recognition)\n",
    "- Semantische Analysen von Texten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80904453-354a-4003-b8df-a863dd6b7223",
   "metadata": {},
   "source": [
    "<i>Abb1</i>: RNN Use-Case Beispiele.\n",
    "\n",
    "<img src=\"./data/img/1_rnn.PNG\" height=800 width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae945969-51f1-4eff-b7ff-129173c7f445",
   "metadata": {},
   "source": [
    "Wieso nutzt man keine normalen Netze für diese Probleme? <br>\n",
    "Alle diese genannten Probleme sind <u>Sequential-Modelling-Problems,</u> weil die <u>Sequenz</u> wichtig ist.\n",
    "\n",
    "Eine Sequenz ist z. B.: \"How are you\", was sich von \"Are you how\" unterscheidet. Für uns Menschen ist das deutlich sichtbar.\n",
    "\n",
    "Weitere Issues mit normale Netzen:\n",
    "- Wie viele Output-Neuronen?\n",
    "- Zu viele Berechnungen, da bei Texten ein Vokabular benutzt wird (Siehe Text in numerischer Darstellung).<br>\n",
    "  Für 30.000 Wörter ist ein Vektor der Größe 30.000 notwendig + Output Größe. <br>\n",
    "  Vokabular:  how: [0, 0, 1, ...], are: [0, 1, 0, ...], ... (On-Hot Encoding)\n",
    "- Keine geteilten Parameter: mehrere Deutungen von Texten.\n",
    "\n",
    "Bei Texten ist die Inputreihenfolge von Wörtern auch entscheidend, nicht wie bei normalen Features wie Eigenschaften, um etwas zu klassifizieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88ea755-69e5-45e1-84e9-0255a7192081",
   "metadata": {},
   "source": [
    "<i>Abb2</i>: Vanilla-RNN Veranschaulichung. Kontext / Memory.\n",
    "\n",
    "<img src=\"./data/img/2_rnn.PNG\" height=800 width=600>\n",
    "\n",
    "\n",
    "<i>Abb3</i>: NER Beispiel.\n",
    "\n",
    "<img src=\"./data/img/3_rnn.PNG\" height=750 width=550>\n",
    "\n",
    "\n",
    "<i>Abb4</i>: ENR Training. \n",
    "\n",
    "<img src=\"./data/img/5_rnn.PNG\" height=700 width=400>\n",
    "\n",
    "\n",
    "<i>Abb5</i>: Generische Darstellung von RNN. \n",
    "\n",
    "<img src=\"./data/img/4_rnn.PNG\" height=160 width=80>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9903ccf-6d0f-4ab8-b8e8-c286a76c18ed",
   "metadata": {},
   "source": [
    "Wie in der Abbildung 2 zusehen ist, wird der Output des Layers wieder zum Input geleitet. Bei Texten kann dann so ein gewisser Kontext mitgenommen werden => die vorherigen Wörter haben einen Einfluss.\n",
    "- Der Zustand wird mitgenommen.\n",
    "\n",
    "Das Training eines RNNs für NER würde so aussehen.:\n",
    "- Nehme Text markiere Namen <br>\n",
    "  \"Bob told me David is home\" => [1, 0, 0, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d236317-3706-4961-9446-22bda67a6172",
   "metadata": {},
   "source": [
    "Wir haben gesehen wie ein einfaches ERN trainiert wird, wie funktioniert das aber bei einer Übersetzung?\n",
    "- Hier müssen alle Wörter vorhanden sein, damit eine Vorhersage gemacht werden kann. Da jedes Wort die Bedeutung und Übersetzung verändert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e358a78-f8e3-498e-aedc-f59f18cbb6b2",
   "metadata": {},
   "source": [
    "<i>Abb5</i>: Übersetzung: Encoder und Decoder.\n",
    "\n",
    "<img src=\"./data/img/6_rnn.PNG\" height=700 width=500>\n",
    "\n",
    "- In diesem Beispiel Single-Layer.\n",
    "\n",
    "\n",
    "<i>Abb7</i>: Deep RNN.\n",
    "\n",
    "<img src=\"./data/img/7_rnn.PNG\" height=600 width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b85090-d477-4971-b913-babb79a41290",
   "metadata": {},
   "source": [
    "<h2>Arten von RNN (3)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1acd58-8ee0-43db-aed9-ea5c98c902e4",
   "metadata": {},
   "source": [
    "RNN Typen.:\n",
    "- Many-to-Many\n",
    "- Many-to-One\n",
    "- One-to-Many"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0a7b9-ea47-4ab6-9419-6bc934854159",
   "metadata": {},
   "source": [
    "Many-to-Many:<br>\n",
    "Viele Inputs und viele Outputs => x(n) und ý(n). Ein Beispiel dafür wäre ENR oder Übersetzungen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf56b08b-5ee4-4150-9744-96a625dfc5c7",
   "metadata": {},
   "source": [
    "<i>Abb8</i>: ENR, Many-to-Many.\n",
    "\n",
    "<img src=\"./data/img/8_rnn.PNG\" height=600 width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf8fa2-ba55-40e0-8356-b2e81420d1c2",
   "metadata": {},
   "source": [
    "Many-to-One:<br>\n",
    "Viele Inputs liefern am Ende ein Output. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d4aa26-f2c6-42a2-879c-e6ca03ecc493",
   "metadata": {},
   "source": [
    "<i>Abb9</i>: Many-to-One.\n",
    "\n",
    "<img src=\"./data/img/9_rnn.PNG\" height=600 width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffb1479-e7e6-452f-bc10-aeeaffc90001",
   "metadata": {},
   "source": [
    "One-to-Many: <br>\n",
    "Für eine Eingabe gibt es viele Outputs. Ein Beispiel dafür wäre Musikgeneration- mit einem gegebenen Seed oder Sample soll eine Melodie generiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c329ab1-b8a2-4247-a590-2017b4ea06c1",
   "metadata": {},
   "source": [
    "<i>Abb10</i>: One-to-Many.\n",
    "\n",
    "<img src=\"./data/img/10_rnn.PNG\" height=600 width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa77c8d-0f03-4ee1-b12e-fb559f6f1642",
   "metadata": {},
   "source": [
    "<h1>Vanising- und Exploding-Gradient</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a00107-4e4f-4094-8a35-fd668d3bc8a9",
   "metadata": {},
   "source": [
    "Das ist einer der Probleme die es bei neuralen Netzen gibt. <br>\n",
    "\n",
    "$\n",
    "weights = weights - Stepsize * gradient\n",
    "$\n",
    "- weights: Model Parameter\n",
    "- Stepsize: Schrittweite des Gradientenabstiegs mit einem gegebenen Verfahren (Siehe SGD, Adam, ...)\n",
    "- Gradient: Berechneter Gradient an der Stelle. <br>\n",
    "  Ableitung Loss- wie sehr sich ein Parameter auf den Fehler am Ende auswirkt <br>\n",
    "  -> Wie verändert sich der Loss bei einer Veränderung der Parameter. \n",
    "\n",
    "Vanishing-Gradient: Veränderungen sind sehr gering => Gewichte ändern sich äußerst gering. <br>\n",
    "Exploding-Gradient: Veränderung sind sehr groß => Gewichte ändern sich mit Updates äußerst stark. \n",
    "\n",
    "Wichtiger Faktor, wenn es um <b>Backpropagation</b> geht.\n",
    "- Siehe: Backpropagation und Kettenregel.\n",
    "\n",
    "Je mehr Layers wir haben, desto mehr Multiplikationen haben wir.\n",
    "- $Gradient = d1 * d2 * ... * dn  $  Bei der Multiplikationen von kleinen Werten $ Gradient = 0.03 * 0.05 = 0.0015$\n",
    "\n",
    "Das zieht sich über das ganze Netzwerk => Parameter der hinteren Layers ändern sich sehr wenig. Der gegenteilige Effekt wäre der Exploding-Gradient\n",
    "\n",
    "Wenn die Anzahl an Layers steigt, wird der Vanising-Gradient immer mehr zum Problem, was das \"Lernen\" negativ beeinflusst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b713b0-4836-4bf2-afba-357a02bbe9e1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Bei RNNs ist Vanising-Gradient Problem eher präsent.\n",
    "\n",
    "\n",
    "Beispiel: Autovervollständigung.<br>\n",
    "Text 1: <u>Heute</u>, wegen meiner Jobsituation und der Familie, <u>muss</u> ich um eine Gehaltserhöhung fragen. <br>\n",
    "Text 2: <u>Letztes Jahr</u>, wegen meiner Jobsituation und der Familie, <u>musste</u> ich um eine Gehaltserhöhung fragen. <br>\n",
    "Dabei ist  <b>\"</b>muss ich um eine Gehaltserhöhung fragen<b>\"</b> der vorhergesagte Text. Das <b>muss</b> und <b>musste</b> bezieht sich auf die Anfangswörter <b>Heute</b> und <b>Letztes Jahr</b>. <br>\n",
    "Wörter die hinten stehen habe ein Einfluss auf Wörter die viel weiter vorne stehen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b023d588-328d-4122-81de-5edcccf88245",
   "metadata": {},
   "source": [
    "<img src=\"./data/img/6_rnn.PNG\" height=700 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2322d876-4d6b-4a42-a49d-0a6e1cc97544",
   "metadata": {},
   "source": [
    "Da die Wörter nacheinander eingegeben werden, wird der Vanising-Gradient mehr zum Problem, da der <u>Effekt</u> der Aktivierung der vergangenen Eingaben z. B. $a^{(2)}$ nachlässt/reduziert  ist, wenn wir schon bei  $a^{(5)}$ sind. Daher kommt der Term <b>Short-Memory.</b> Dadurch ist das Predicten der Wörter <b>muss</b> und <b>musste</b> schwierig.\n",
    "\n",
    "Vanilla-RNNs haben ein kurzes Gedächtnis.\n",
    "- Genau dieses Problem des Vanising-Gradient ist die Ursache für das Short-Memory Problem.\n",
    "- Durch Backpropagation ändern sich die Parameter geringfügig, dadurch kommt dieser Effekt zustande.\n",
    "\n",
    "Als Lösung: LSTM und GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198b887-30a2-4bb8-8496-1fe055f6801c",
   "metadata": {},
   "source": [
    "<h1>LSTM und GRU</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb651be-b2aa-4b72-bcf7-b8fbdb0f2e21",
   "metadata": {},
   "source": [
    "Als Lösung für das Probem mit Short-Memory wird LSTM und GRU als Ansatz genutzt.\n",
    "\n",
    "Als Beispiel wieder eine Autovervollständigung. <br>\n",
    "Text 1: <u>Heute</u>, wegen meiner Jobsituation und der Familie, <u>muss</u> ich um eine Gehaltserhöhung fragen. <br>\n",
    "Text 2: <u>Letztes Jahr</u>, wegen meiner Jobsituation und der Familie, <u>musste</u> ich um eine Gehaltserhöhung fragen. <br>\n",
    "\n",
    "Um <b>muss</b> und <b>musste</b> vorherzusagen, muss sich das Model an <u>Heute</u> und <u>Letztes Jahr</u> \"Erinnern\". Normale RNNs können sich wegen dem Vanishing-Gradient nur an einige vergangene Worte erinnern.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c8ccb0-fdfa-4c7e-86af-d592fe9266ca",
   "metadata": {},
   "source": [
    "<i>Abb11</i>: Short- und Long Term Zellen.\n",
    "\n",
    "<img src=\"./data/img/11_rnn.PNG\" height=500 width=800>\n",
    "\n",
    "<i>Abb12</i>: Traditionelle darstellung einer einfachen RNN Zelle.\n",
    "\n",
    "<img src=\"./data/img/12_rnn.PNG\" height=400 width=500>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4dc9fd29-0d6b-4307-88d7-39976271b0ef",
   "metadata": {},
   "source": [
    "Um auch ein Long Term Memory zu nutzen, wird ein neuer Zustand vorgestellt: c\n",
    "\n",
    "<b>h:</b> Hidden State <br>\n",
    "<b>c:</b> Cell State\n",
    "\n",
    "In der Abbildung 11 sieht man nochmal ein detaillierteren Aufbau einer normalen RNN Zelle.  $h^{(t-1)}$ steht hier für den vorherigen Zustand der mit eingegeben wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159bd32f-9cd9-4e4f-aaf1-4c43f26c900a",
   "metadata": {},
   "source": [
    "LSTM ist bei längeren Sequenzen genauer, aber weniger effizient. GRU ist berechnungstechnisch effizienter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637a62b4-14f0-4598-ac1e-e855866c2140",
   "metadata": {},
   "source": [
    "<h2>LSTM</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eed6a76-36f4-486d-a57e-b4a0f2580787",
   "metadata": {},
   "source": [
    "LSTM steht für Long Short Term Memory. Hier nutzen wir zusätzliche Mechanismen, um ein besseres Gedächtnis zu implementieren.\n",
    "\n",
    "Bei Texten sind auch bestimmte Key-Words wichtig, die z. B. dabei helfen vorherzusagen, das man aus Indien kommt, wenn man verschiedene Traditionelle indische Gericht erwähnt oder zubereitet.\n",
    "- \".... Biryani, so my sister is also\" -> Indian. <br>\n",
    "  Dadurch sind andere Wörter weniger wichtig => Welcher Wörter sind für die weitere Vorhersage ausschlaggebend?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a62af4-700d-48bd-a49c-94d9f1d05c4f",
   "metadata": {},
   "source": [
    "Die folgende Abbildung zeigt wie ein Text mit einem normalen RNN und einem LSTM verarbeitet wird.\n",
    "- Als Beispiel kann sich das RNN nur 2 Wörter merken, in Wirklichkeit mehr.\n",
    "  \n",
    "Der Text: \"Ram eats <u>samosa</u> everyday, it shouldn't be hard to guess that his favorite cuisine is <u>Inidan</u>\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24054a0-df3c-486c-adba-55c98fffbccf",
   "metadata": {},
   "source": [
    "<i>Abb13</i>: Texteingabe in RNN und LSTM: merke Key-Words.\n",
    "\n",
    "<img src=\"./data/img/13_rnn.PNG\" height=400 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51777d8f-8b2b-413c-9c20-31866f0fbe3a",
   "metadata": {},
   "source": [
    "Bei dem RNN nach dem <b>is</b> hat es keine Erinnerung mehr an <b>samosa</b>. Das mach die Vorhersage schiwerig das die <b>cuisine</b> <u>Inidan</u> ist.<br>\n",
    "Bei dem LSTM, wenn jetzt eine Prediction bei Cuisine machen müssen, haben wir dank dem Speichers das Keyword <b>samosa</b> gemerkt, wir haben einen Kontext und können <b>Inidan</b> ausgeben.\n",
    "\n",
    "Mit dem Zusammenspiel eines Long Term Memory mit einem Short Term Memory wollen wir die <u>Keywords / bedeutsamen Wörter</u> in dem Langzeitgedächtnis speichern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb26b6-e67c-411a-8642-6a0a6323e1c6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Ein anderes Textbeispiel:<br>\n",
    "Text: \"Ram eats samosa everyday, it shouldn't be hard to guess that his favorite cuisine is Inidan. His brother Pavan however is a lover of <b>pasta</b> and <b>cheese</b> that means Pavan's favorite cuisine is <b>Italian</b>\"\n",
    "- Keywords: pasta und cheese <br>\n",
    "  -> Italian\n",
    "\n",
    "Hier wird im Langzeitgedächtnis das Wort \"samosa\" gespeichert bis wir auf das Wort \"pasta\" treffen. Dann muss \"samosa\" <u>vergessen</u> werden, um Platz für \"pasta\" zu schaffen. <br>\n",
    "Auf den weiteren Weg nehmen wir noch \"cheese\" mit und speichern es ebenfalls (addiere hinzu). Am Ende, wenn das Model gefragt wird, sollte es \"Italian\" ausgeben.\n",
    "\n",
    "Während des Trainings wird für sowas eine Art Gespür entwickelt. <br>\n",
    "Dadurch lern das Model was wichtig und weniger wichtig ist => Was speichern und was vergessen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c5619b-b63c-4b13-9a4e-be6f3131be91",
   "metadata": {},
   "source": [
    "<i>Abb14</i>: LSTM Zelle Schaubild.\n",
    "\n",
    "<img src=\"./data/img/14_rnn.PNG\" height=700 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16424422-17bf-4e6e-a387-8334c9612061",
   "metadata": {},
   "source": [
    "Einer der wichtigen Bestandteile eines LSTMs ist das <b>Forget Gate</b>. Wenn es das derzeitige Gedächtnis leeren soll, wird ein Vektor ausgegeben deren Werte nahe 0 sind. Wenn dieser Vektor mit dem derzeitigen Gedächtnis multipliziert wird, ist der Ergebnis 0 oder nahe 0.\n",
    "\n",
    "Dann gibt es noch das <b>Input Gate</b>. Wenn wir ein Wort vergessen wollen z. B. \"samosa\", weil wir \"pasta\" hinzufügen, nutzen wir das Input Gate, um diese Information abzuspeichern. Das Input Gate multipliziert zwei Vektoren, wie auf dem Bild 14 zusehen ist.\n",
    "\n",
    "Die dritte Komponente ist das <b>Output Gate</b>. Hier werden auch zwei Vektoren multipliziert, das Ergebnis wird dann nach außen geleitet als y. Dabei wird auch der <b>Hidden State</b> gesetzt, was beim Input wieder verwendet wird.\n",
    "- Wobei y dasselbe ist wie $h^{(t)}$. y wird hier für Ausgaben genutzt wenn diese z. B. für ein Softmax Layer benötigt werden.\n",
    "\n",
    "Man sieht das der Aufbau deutlich aufwendiger ist. Dank Frameworks könnne wir diese einfach nutzen. \n",
    "\n",
    "Das Long Term Memory nutzt also 2 Komponenten für das Gedächtnis, das Forget Gate und das Input Gate. Das Forget Gate hilf dabei Wörter wie \"samosa\" zu vergessen, wenn Wörter wie \"pasta\" kommen. Das Input Gate hilft dabei <u>bedeutsame</u> Wörter zu speichern. Während des Trainings lernt das Model, worauf es ankommt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf6587-6bbc-424e-8098-23dc4453234e",
   "metadata": {},
   "source": [
    "<h2>GRU</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a31cff-8701-4e0c-bcd2-4fa6d7a6cebb",
   "metadata": {},
   "source": [
    "GRU steht für Gated Recurrent Unit (2014) und ist jünger als LSTM. Wie auch bei LSTM ist das Ziel das Problem mit den Short Term Memory zu lösen. GRU ist eine modifizierte und leichtere Variante von LSTM. \n",
    "\n",
    "GRU <b>kombiniert</b> Long und Short Term Memory in einem einzigen Zustand, dem Hidden State, was einer der Kernunterschiede ist. GRU nutzt nur zwei Gates anders als LSTM drei Gates.\n",
    "- Update Gate: Wie viel von der Vergangenheit gemerkt werden soll.\n",
    "- Reset Gate: Wie viele von der Vergangenehit vergessen werden soll.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4909b628-8fb2-46af-9f46-2e5ed7896897",
   "metadata": {},
   "source": [
    "<i>Abb15</i>: GRU Zelle Schaubild.\n",
    "\n",
    "<img src=\"./data/img/15_rnn.PNG\" height=700 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b2ae4b-4485-42f1-a393-791acf264e81",
   "metadata": {},
   "source": [
    "Wenn wir also das Wort \"pasta\" haben, muss das Wort \"samosa\" vergessen werden. Diese Änderung erfolgt über das <b>Reset Gate</b>. Wie auf der Abbildung 15 werden hierzu zwei Vektoren vorerst addiert, bevor eine Aktivierungsfunktion angewendet wird.\n",
    "\n",
    "\n",
    "Wenn dann das Wort \"Cheese\" kommt, wollen wir die Erinnerung an \"pasta\" behalten. Dafür nutzen wir das <b>Update Gate</b>. Der Vorgang ist gleich- addiere Vektoren und wende Aktivierungsfunktion an. Der einzige Unterschied ist das es andere Weights sind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d61c0-e75e-40fa-9c5e-c98d1ba91fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de477c51-d53b-4ffd-b305-0a125edf0d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a9e28-6555-4d10-b7b6-fd5cd5790e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba3924-e9d0-43fb-acce-6b399bf11b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daa8cc8-8922-45fa-8559-c13f9ede4912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
